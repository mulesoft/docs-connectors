= Apache Kafka Connector 4.4 Reference - Mule 4
:page-aliases: connectors::kafka/kafka-connector-reference.adoc

Support Category: https://www.mulesoft.com/legal/versioning-back-support-policy#anypoint-connectors[Select]

Anypoint Connector for Apache Kafka (Apache Kafka Connector) enables you to interact with the Apache Kafka messaging system. It provides seamless integration between your Mule app and an Apache Kafka cluster, using Mule runtime engine (Mule).

Release Notes: xref:release-notes::connector/kafka-connector-release-notes-mule-4.adoc[Apache Kafka Connector]


== Configurations
---
[[consumer-config]]
=== Consumer configuration


==== Parameters
[cols=".^20%,.^20%,.^35%,.^20%,^.^5%", options="header"]
|======================
| Name | Type | Description | Default Value | Required
|Name | String | The name for this configuration. Connectors reference the configuration with this name. | | *x*{nbsp}
| Connection a| * <<consumer-config_consumer-plaintext-connection, Consumer Plaintext Connection>> {nbsp}
* <<consumer-config_consumer-sasl-gssapi-connection, Consumer Kerberos Connection>> {nbsp}
* <<consumer-config_consumer-sasl-plain-connection, Consumer SASL/PLAIN Connection>> {nbsp}
* <<consumer-config_consumer-sasl-scram-connection, Consumer SASL/SCRAM Connection>> {nbsp}
 | The connection types that can be provided to this configuration. | | *x*{nbsp}
| Default acknowledgement mode a| Enumeration, one of:

** AUTO
** MANUAL
** IMMEDIATE
** DUPS_OK |  +++Defines the way that the Kafka Broker instance is notified of the consumption of messages. AUTO: Messages are committed only if the flow is finished successfully. MANUAL: The user must commit manually through the Commit operation. IMMEDIATE: Mule automatically commits the messages upon reception and before triggering the flow. DUPS_OK: Same as the MANUAL mode, but the commit is made asynchronously, which can lead to duplicate records.+++ |  +++AUTO+++ | {nbsp}
| Default listener poll timeout a| Number |  +++The time, in time units, spent waiting to do a poll if data is not available in the buffer (fetched). If no value is set, returns immediately with any records that are available currently in the buffer, else returns empty. Must not be negative.+++ |  +++100+++ | {nbsp}
| Default listener poll timeout time unit a| Enumeration, one of:

** NANOSECONDS
** MICROSECONDS
** MILLISECONDS
** SECONDS
** MINUTES
** HOURS
** DAYS |  +++The time unit for the polling timeout. This combines with pollTimeout to define the total timeout for the polling.+++ |  +++MILLISECONDS+++ | {nbsp}
| Default listener poll timeout a| Number |  +++The time, in time units, spent waiting for an operation to finish. If no value is set or a negative value is set, the operation will wait forever. Must not be negative.+++ |  +++-1+++ | {nbsp}
| Default operation timeout time unit a| Enumeration, one of:

** NANOSECONDS
** MICROSECONDS
** MILLISECONDS
** SECONDS
** MINUTES
** HOURS
** DAYS |  +++The time unit for the operation timeout. This combines with operationTimeout to define the total default timeout for the operations that use this config.+++ |  +++SECONDS+++ | {nbsp}
| Zone ID a| String |  +++Zone ID used to convert the provided timestamps into ZonedLocalDateTimes in the results. Default value is the system one.+++ |  | {nbsp}
| Expiration Policy a| <<ExpirationPolicy>> |  +++Configures the minimum amount of time that a dynamic configuration instance can remain idle before the runtime considers it eligible for expiration. This does not mean that the platform will expire the instance at the exact moment that it becomes eligible. The runtime will actually purge the instances when it sees it fit.+++ |  | {nbsp}
|======================

==== Connection Types
[[consumer-config_consumer-plaintext-connection]]
===== Consumer Plaintext Connection


====== Parameters
[cols=".^20%,.^20%,.^35%,.^20%,^.^5%", options="header"]
|======================
| Name | Type | Description | Default Value | Required
| Bootstrap Server URLs a| Array of String |  +++The list of servers to bootstrap the connection with the kafka cluster. This can be a partial list of the available servers.+++ |  | *x*{nbsp}
| Endpoint identification algorithm a| String |  +++The endpoint identification algorithm used by clients to validate server host name. The default value is an empty string, which means it is disabled. Clients including client connections created by the broker for inter-broker communication verify that the broker host name matches the host name in the brokers certificate.+++ |  | {nbsp}
| Group ID a| String |  +++Default Group ID for all the Kafka Consumers that use this configuration.+++ |  | {nbsp}
| Consumer Amount a| Number |  +++Determines the number of consumers the connection will initially create.+++ |  +++1+++ | {nbsp}
| Maximum polling interval a| Number |  +++The configuration controls the maximum amount of time the client will wait for the response of a request. If the response is not received before the timeout elapses the client will resend the request if necessary or fail the request if retries are exhausted. This parameter can be overridden at source level.+++ |  +++60+++ | {nbsp}
| Maximum Polling Interval Time Unit a| Enumeration, one of:

** NANOSECONDS
** MICROSECONDS
** MILLISECONDS
** SECONDS
** MINUTES
** HOURS
** DAYS |  +++Determines the time unit for request timeout scalar. This parameter can be overridden at source level.+++ |  +++SECONDS+++ | {nbsp}
| Isolation Level a| Enumeration, one of:

** READ_UNCOMMITTED
** READ_COMMITTED |  +++Controls how to read messages written transactionally. If set to <code>read_committed</code>, consumer.poll() will only return" + " transactional messages which have been committed. If set to <code>read_uncommitted</code>' (the default), consumer.poll() will return all messages, even transactional messages" + " which have been aborted. Non-transactional messages will be returned unconditionally in either mode.</p> <p>Messages will always be returned in offset order. Hence, in " + " <code>read_committed</code> mode, consumer.poll() will only return messages up to the last stable offset (LSO), which is the one less than the offset of the first open transaction." + " In particular any messages appearing after messages belonging to ongoing transactions will be withheld until the relevant transaction has been completed. As a result, <code>read_committed</code>" + " consumers will not be able to read up to the high watermark when there are in flight transactions.</p><p> Further, when in <code>read_committed</code> the seekToEnd method will" + " return the LSO+++ |  +++READ_UNCOMMITTED+++ | {nbsp}
| Exclude internal topics a| Boolean |  +++Whether internal topics matching a subscribed pattern should be excluded from the subscription. It is always possible to explicitly subscribe to an internal topic.+++ |  +++true+++ | {nbsp}
| Auto offset reset a| Enumeration, one of:

** EARLIEST
** LATEST
** ERROR |  +++What to do when there is no initial offset in Kafka or if the current offset does not exist any more on the server (e.g. because that data has been deleted): EARLIEST: automatically reset the offset to the earliest offset. LATEST: automatically reset the offset to the latest offset. ERROR: throw error to the if no previous offset is found for the consumer's group.+++ |  +++LATEST+++ | {nbsp}
| Retry Backoff Timeout a| Number |  +++The amount of time to wait before attempting to retry a failed request to a given topic partition. This avoids repeatedly sending requests in a tight loop under some failure scenarios.+++ |  +++100+++ | {nbsp}
| Retry Backoff Timeout Time Unit a| Enumeration, one of:

** NANOSECONDS
** MICROSECONDS
** MILLISECONDS
** SECONDS
** MINUTES
** HOURS
** DAYS |  +++Determines the time unit for the reconnect backoff timeout scalar.+++ |  +++MILLISECONDS+++ | {nbsp}
| Check CRC a| Boolean |  +++Automatically check the CRC32 of the records consumed. This ensures no on-the-wire or on-disk corruption to the messages occurred. This check adds some overhead, so it may be disabled in cases seeking extreme performance.+++ |  +++true+++ | {nbsp}
| Default receive buffer size a| Number |  +++The size of the TCP receive buffer (SO_RCVBUF) to use when reading data. If the value is -1, the OS default will be used. This parameter can be overridden at source level.+++ |  +++64+++ | {nbsp}
| Default receive buffer size unit a| Enumeration, one of:

** BYTE
** KB
** MB
** GB |  +++The unit of measure for the receive buffer size scalar. This parameter can be overridden at source level.+++ |  +++KB+++ | {nbsp}
| Default send buffer size a| Number |  +++The size of the TCP send buffer (SO_SNDBUF) to use when sending data. If the value is -1, the OS default will be used. This parameter can be overridden at source level.+++ |  +++128+++ | {nbsp}
| Default send buffer size unit a| Enumeration, one of:

** BYTE
** KB
** MB
** GB |  +++The unit of measure for the send buffer size scalar. This parameter can be overridden at source level.+++ |  +++KB+++ | {nbsp}
| Request Timeout a| Number |  +++The configuration controls the maximum amount of time the client will wait for the response of a request. If the response is not received before the timeout elapses the client will resend the request if necessary or fail the request if retries are exhausted. This parameter can be overridden at source level.+++ |  +++30+++ | {nbsp}
| Request Timeout Time Unit a| Enumeration, one of:

** NANOSECONDS
** MICROSECONDS
** MILLISECONDS
** SECONDS
** MINUTES
** HOURS
** DAYS |  +++Determines the time unit for request timeout scalar. This parameter can be overridden at source level.+++ |  +++SECONDS+++ | {nbsp}
| Default record limit a| Number |  +++The maximum number of records returned on a poll call to the Kafka cluster. This parameter can be overridden at source level.+++ |  +++500+++ | {nbsp}
| DNS Lookups a| Enumeration, one of:

** DEFAULT
** USE_ALL_DNS_IPS
** RESOLVE_CANONICAL_BOOTSTRAP_SERVERS_ONLY |  +++Controls how the client uses DNS lookups. If set to use_all_dns_ips then, when the lookup returns multiple IP addresses for a hostname, they will all be attempted to connect to before failing the connection. Applies to both bootstrap and advertised servers. If the value is resolve_canonical_bootstrap_servers_only each entry will be resolved and expanded into a list of canonical names.+++ |  +++DEFAULT+++ | {nbsp}
| Heartbeat interval a| Number |  +++The expected time between heartbeats to the consumer coordinator when using Kafka's group management facilities. Heartbeats are used to ensure that the consumer's session stays active and to facilitate rebalancing when new consumers join or leave the group. The value must be set lower than session.timeout.ms, but typically should be set no higher than 1/3 of that value. It can be adjusted even lower to control the expected time for normal rebalances.+++ |  +++3+++ | {nbsp}
| Heartbeat Interval Time Unit a| Enumeration, one of:

** NANOSECONDS
** MICROSECONDS
** MILLISECONDS
** SECONDS
** MINUTES
** HOURS
** DAYS |  +++Determines the time unit for fetch heartbeat interval time scalar.+++ |  +++SECONDS+++ | {nbsp}
| Session timeout a| Number |  +++The timeout used to detect consumer failures when using Kafka's group management facility. The consumer sends periodic heartbeats to indicate its liveness to the broker. If no heartbeats are received by the broker before the expiration of this session timeout, then the broker will remove this consumer from the group and initiate a rebalance. Note that the value must be in the allowable range as configured in the broker configuration by group.min.session.timeout.ms and group.max.session.timeout.ms.+++ |  +++10+++ | {nbsp}
| Session timeout time unit a| Enumeration, one of:

** NANOSECONDS
** MICROSECONDS
** MILLISECONDS
** SECONDS
** MINUTES
** HOURS
** DAYS |  +++Determines the time unit for session timeout scalar.+++ |  +++SECONDS+++ | {nbsp}
| Connection maximum idle time a| Number |  +++Close idle connections after the number of milliseconds specified by this config.+++ |  +++540+++ | {nbsp}
| Connection maximum idle time time unit a| Enumeration, one of:

** NANOSECONDS
** MICROSECONDS
** MILLISECONDS
** SECONDS
** MINUTES
** HOURS
** DAYS |  +++Determines the time unit for connections maximum idle time scalar.+++ |  +++SECONDS+++ | {nbsp}
| TLS Configuration a| <<Tls>> |  +++Defines a TLS configuration, which can be used from both the client and server sides to secure communication for the Mule app. The connector will automatically set the 'security.protocol' to use for communication. Valid values are PLAINTEXT / SSL / SASL_PLAINTEXT / SASL_SSL. Default value when no configuration has been provided is PLAINTEXT(or SASL_PLAINTEXT for SASL authentication - kerberos/scram/plain). If SSL was configured as protocol on the broker side then the user needs to configure at least the keystore in the 'tls:context' child element of this config and the connector will automatically use SSL(or SASL_SSL for SASL authentication) as 'security.protocol'.+++ |  | {nbsp}
| Topic Subscription Patterns a| Array of String |  +++The list of subscription regular expressions to subscribe to. This topics will be automatically rebalanced between the amount of consumers of the topic.+++ |  | {nbsp}
| Assignments a| Array of <<TopicPartition>> |  +++The list of topic-partition pairs to assign. Note that there will be no automatic rebalance of the consumers +++ |  | {nbsp}
| Default fetch minimum size a| Number |  +++The minimum amount of data the server should return for a fetch request. If insufficient data is available the request will wait for that much data to accumulate before answering the request. The default setting of 1 byte means that fetch requests are answered as soon as a single byte of data is available or the fetch request times out waiting for data to arrive. Setting this to something greater than 1 will cause the server to wait for larger amounts of data to accumulate which can improve server throughput a bit at the cost of some additional latency. This parameter can be overridden at source level.+++ |  +++1+++ | {nbsp}
| Fetch Minimum Size Unit a| Enumeration, one of:

** BYTE
** KB
** MB
** GB |  |  +++BYTE+++ | {nbsp}
| Default fetch maximum size a| Number |  +++The maximum amount of data the server should return for a fetch request. Records are fetched in batches by the consumer, and if the first record batch in the first non-empty partition of the fetch is larger than this value, the record batch will still be returned to ensure that the consumer can make progress. As such, this is not an absolute maximum. The maximum record batch size accepted by the broker is defined via message.max.bytes (broker config) or max.message.bytes (topic config). Note that the consumer performs multiple fetches in parallel. This parameter can be overridden at source level.+++ |  +++1+++ | {nbsp}
| Default maximum fetch size unit a| Enumeration, one of:

** BYTE
** KB
** MB
** GB |  +++The unit of measure for the maximum partition fetch size scalar. This parameter can be overridden at source level.+++ |  +++MB+++ | {nbsp}
| Default maximum partition fetch size a| Number |  +++The maximum amount of data per-partition the server will return. Records are fetched in batches by the consumer. If the first record batch in the first non-empty partition of the fetch is larger than this limit, the batch will still be returned to ensure that the consumer can make progress. The maximum record batch size accepted by the broker is defined via message.max.bytes (broker config) or max.message.bytes (topic config). See fetch.max.bytes for limiting the consumer request size.This parameter can be overridden at source level.+++ |  +++1+++ | {nbsp}
| Default maximum partition fetch unit a| Enumeration, one of:

** BYTE
** KB
** MB
** GB |  +++The unit of measure for the maximum partition fetch size scalar. This parameter can be overridden at source level.+++ |  +++MB+++ | {nbsp}
| Fetch Maximum Wait Timeout a| Number |  +++The maximum amount of time the server will block before answering the fetch request if there isn't sufficient data to immediately satisfy the requirement given by fetch.min.bytes.+++ |  +++500+++ | {nbsp}
| Fetch Maximum Wait Timeout Unit a| Enumeration, one of:

** NANOSECONDS
** MICROSECONDS
** MILLISECONDS
** SECONDS
** MINUTES
** HOURS
** DAYS |  +++Determines the time unit for fetch maximum wait timeout scalar.+++ |  +++MILLISECONDS+++ | {nbsp}
| Reconnection a| <<Reconnection>> |  +++When the application is deployed, a connectivity test is performed on all connectors. If set to true, deployment will fail if the test doesn't pass after exhausting the associated reconnection strategy+++ |  | {nbsp}
|======================
[[consumer-config_consumer-sasl-gssapi-connection]]
===== Consumer Kerberos Connection


====== Parameters
[cols=".^20%,.^20%,.^35%,.^20%,^.^5%", options="header"]
|======================
| Name | Type | Description | Default Value | Required
| Bootstrap Server URLs a| Array of String |  +++The list of servers to bootstrap the connection with the kafka cluster. This can be a partial list of the available servers.+++ |  | *x*{nbsp}
| Endpoint identification algorithm a| String |  +++The endpoint identification algorithm used by clients to validate server host name. The default value is an empty string, which means it is disabled. Clients including client connections created by the broker for inter-broker communication verify that the broker host name matches the host name in the brokers certificate.+++ |  | {nbsp}
| Group ID a| String |  +++Default Group ID for all the Kafka Consumers that use this configuration.+++ |  | {nbsp}
| Consumer Amount a| Number |  +++Determines the number of consumers the connection will initially create.+++ |  +++1+++ | {nbsp}
| Maximum polling interval a| Number |  +++The configuration controls the maximum amount of time the client will wait for the response of a request. If the response is not received before the timeout elapses the client will resend the request if necessary or fail the request if retries are exhausted. This parameter can be overridden at source level.+++ |  +++60+++ | {nbsp}
| Maximum Polling Interval Time Unit a| Enumeration, one of:

** NANOSECONDS
** MICROSECONDS
** MILLISECONDS
** SECONDS
** MINUTES
** HOURS
** DAYS |  +++Determines the time unit for request timeout scalar. This parameter can be overridden at source level.+++ |  +++SECONDS+++ | {nbsp}
| Isolation Level a| Enumeration, one of:

** READ_UNCOMMITTED
** READ_COMMITTED |  +++Controls how to read messages written transactionally. If set to <code>read_committed</code>, consumer.poll() will only return" + " transactional messages which have been committed. If set to <code>read_uncommitted</code>' (the default), consumer.poll() will return all messages, even transactional messages" + " which have been aborted. Non-transactional messages will be returned unconditionally in either mode.</p> <p>Messages will always be returned in offset order. Hence, in " + " <code>read_committed</code> mode, consumer.poll() will only return messages up to the last stable offset (LSO), which is the one less than the offset of the first open transaction." + " In particular any messages appearing after messages belonging to ongoing transactions will be withheld until the relevant transaction has been completed. As a result, <code>read_committed</code>" + " consumers will not be able to read up to the high watermark when there are in flight transactions.</p><p> Further, when in <code>read_committed</code> the seekToEnd method will" + " return the LSO+++ |  +++READ_UNCOMMITTED+++ | {nbsp}
| Exclude internal topics a| Boolean |  +++Whether internal topics matching a subscribed pattern should be excluded from the subscription. It is always possible to explicitly subscribe to an internal topic.+++ |  +++true+++ | {nbsp}
| Auto offset reset a| Enumeration, one of:

** EARLIEST
** LATEST
** ERROR |  +++What to do when there is no initial offset in Kafka or if the current offset does not exist any more on the server (e.g. because that data has been deleted): EARLIEST: automatically reset the offset to the earliest offset. LATEST: automatically reset the offset to the latest offset. ERROR: throw error to the if no previous offset is found for the consumer's group.+++ |  +++LATEST+++ | {nbsp}
| Retry Backoff Timeout a| Number |  +++The amount of time to wait before attempting to retry a failed request to a given topic partition. This avoids repeatedly sending requests in a tight loop under some failure scenarios.+++ |  +++100+++ | {nbsp}
| Retry Backoff Timeout Time Unit a| Enumeration, one of:

** NANOSECONDS
** MICROSECONDS
** MILLISECONDS
** SECONDS
** MINUTES
** HOURS
** DAYS |  +++Determines the time unit for the reconnect backoff timeout scalar.+++ |  +++MILLISECONDS+++ | {nbsp}
| Check CRC a| Boolean |  +++Automatically check the CRC32 of the records consumed. This ensures no on-the-wire or on-disk corruption to the messages occurred. This check adds some overhead, so it may be disabled in cases seeking extreme performance.+++ |  +++true+++ | {nbsp}
| Default receive buffer size a| Number |  +++The size of the TCP receive buffer (SO_RCVBUF) to use when reading data. If the value is -1, the OS default will be used. This parameter can be overridden at source level.+++ |  +++64+++ | {nbsp}
| Default receive buffer size unit a| Enumeration, one of:

** BYTE
** KB
** MB
** GB |  +++The unit of measure for the receive buffer size scalar. This parameter can be overridden at source level.+++ |  +++KB+++ | {nbsp}
| Default send buffer size a| Number |  +++The size of the TCP send buffer (SO_SNDBUF) to use when sending data. If the value is -1, the OS default will be used. This parameter can be overridden at source level.+++ |  +++128+++ | {nbsp}
| Default send buffer size unit a| Enumeration, one of:

** BYTE
** KB
** MB
** GB |  +++The unit of measure for the send buffer size scalar. This parameter can be overridden at source level.+++ |  +++KB+++ | {nbsp}
| Request Timeout a| Number |  +++The configuration controls the maximum amount of time the client will wait for the response of a request. If the response is not received before the timeout elapses the client will resend the request if necessary or fail the request if retries are exhausted. This parameter can be overridden at source level.+++ |  +++30+++ | {nbsp}
| Request Timeout Time Unit a| Enumeration, one of:

** NANOSECONDS
** MICROSECONDS
** MILLISECONDS
** SECONDS
** MINUTES
** HOURS
** DAYS |  +++Determines the time unit for request timeout scalar. This parameter can be overridden at source level.+++ |  +++SECONDS+++ | {nbsp}
| Default record limit a| Number |  +++The maximum number of records returned on a poll call to the Kafka cluster. This parameter can be overridden at source level.+++ |  +++500+++ | {nbsp}
| DNS Lookups a| Enumeration, one of:

** DEFAULT
** USE_ALL_DNS_IPS
** RESOLVE_CANONICAL_BOOTSTRAP_SERVERS_ONLY |  +++Controls how the client uses DNS lookups. If set to use_all_dns_ips then, when the lookup returns multiple IP addresses for a hostname, they will all be attempted to connect to before failing the connection. Applies to both bootstrap and advertised servers. If the value is resolve_canonical_bootstrap_servers_only each entry will be resolved and expanded into a list of canonical names.+++ |  +++DEFAULT+++ | {nbsp}
| Heartbeat interval a| Number |  +++The expected time between heartbeats to the consumer coordinator when using Kafka's group management facilities. Heartbeats are used to ensure that the consumer's session stays active and to facilitate rebalancing when new consumers join or leave the group. The value must be set lower than session.timeout.ms, but typically should be set no higher than 1/3 of that value. It can be adjusted even lower to control the expected time for normal rebalances.+++ |  +++3+++ | {nbsp}
| Heartbeat Interval Time Unit a| Enumeration, one of:

** NANOSECONDS
** MICROSECONDS
** MILLISECONDS
** SECONDS
** MINUTES
** HOURS
** DAYS |  +++Determines the time unit for fetch heartbeat interval time scalar.+++ |  +++SECONDS+++ | {nbsp}
| Session timeout a| Number |  +++The timeout used to detect consumer failures when using Kafka's group management facility. The consumer sends periodic heartbeats to indicate its liveness to the broker. If no heartbeats are received by the broker before the expiration of this session timeout, then the broker will remove this consumer from the group and initiate a rebalance. Note that the value must be in the allowable range as configured in the broker configuration by group.min.session.timeout.ms and group.max.session.timeout.ms.+++ |  +++10+++ | {nbsp}
| Session timeout time unit a| Enumeration, one of:

** NANOSECONDS
** MICROSECONDS
** MILLISECONDS
** SECONDS
** MINUTES
** HOURS
** DAYS |  +++Determines the time unit for session timeout scalar.+++ |  +++SECONDS+++ | {nbsp}
| Connection maximum idle time a| Number |  +++Close idle connections after the number of milliseconds specified by this config.+++ |  +++540+++ | {nbsp}
| Connection maximum idle time time unit a| Enumeration, one of:

** NANOSECONDS
** MICROSECONDS
** MILLISECONDS
** SECONDS
** MINUTES
** HOURS
** DAYS |  +++Determines the time unit for connections maximum idle time scalar.+++ |  +++SECONDS+++ | {nbsp}
| TLS Configuration a| <<Tls>> |  +++Defines a TLS configuration, which can be used from both the client and server sides to secure communication for the Mule app. The connector will automatically set the 'security.protocol' to use for communication. Valid values are PLAINTEXT / SSL / SASL_PLAINTEXT / SASL_SSL. Default value when no configuration has been provided is PLAINTEXT(or SASL_PLAINTEXT for SASL authentication - kerberos/scram/plain). If SSL was configured as protocol on the broker side then the user needs to configure at least the keystore in the 'tls:context' child element of this config and the connector will automatically use SSL(or SASL_SSL for SASL authentication) as 'security.protocol'.+++ |  | {nbsp}
| Topic Subscription Patterns a| Array of String |  +++The list of subscription regular expressions to subscribe to. This topics will be automatically rebalanced between the amount of consumers of the topic.+++ |  | {nbsp}
| Assignments a| Array of <<TopicPartition>> |  +++The list of topic-partition pairs to assign. Note that there will be no automatic rebalance of the consumers +++ |  | {nbsp}
| Default fetch minimum size a| Number |  +++The minimum amount of data the server should return for a fetch request. If insufficient data is available the request will wait for that much data to accumulate before answering the request. The default setting of 1 byte means that fetch requests are answered as soon as a single byte of data is available or the fetch request times out waiting for data to arrive. Setting this to something greater than 1 will cause the server to wait for larger amounts of data to accumulate which can improve server throughput a bit at the cost of some additional latency. This parameter can be overridden at source level.+++ |  +++1+++ | {nbsp}
| Fetch Minimum Size Unit a| Enumeration, one of:

** BYTE
** KB
** MB
** GB |  |  +++BYTE+++ | {nbsp}
| Default fetch maximum size a| Number |  +++The maximum amount of data the server should return for a fetch request. Records are fetched in batches by the consumer, and if the first record batch in the first non-empty partition of the fetch is larger than this value, the record batch will still be returned to ensure that the consumer can make progress. As such, this is not an absolute maximum. The maximum record batch size accepted by the broker is defined via message.max.bytes (broker config) or max.message.bytes (topic config). Note that the consumer performs multiple fetches in parallel. This parameter can be overridden at source level.+++ |  +++1+++ | {nbsp}
| Default maximum fetch size unit a| Enumeration, one of:

** BYTE
** KB
** MB
** GB |  +++The unit of measure for the maximum partition fetch size scalar. This parameter can be overridden at source level.+++ |  +++MB+++ | {nbsp}
| Default maximum partition fetch size a| Number |  +++The maximum amount of data per-partition the server will return. Records are fetched in batches by the consumer. If the first record batch in the first non-empty partition of the fetch is larger than this limit, the batch will still be returned to ensure that the consumer can make progress. The maximum record batch size accepted by the broker is defined via message.max.bytes (broker config) or max.message.bytes (topic config). See fetch.max.bytes for limiting the consumer request size.This parameter can be overridden at source level.+++ |  +++1+++ | {nbsp}
| Default maximum partition fetch unit a| Enumeration, one of:

** BYTE
** KB
** MB
** GB |  +++The unit of measure for the maximum partition fetch size scalar. This parameter can be overridden at source level.+++ |  +++MB+++ | {nbsp}
| Fetch Maximum Wait Timeout a| Number |  +++The maximum amount of time the server will block before answering the fetch request if there isn't sufficient data to immediately satisfy the requirement given by fetch.min.bytes.+++ |  +++500+++ | {nbsp}
| Fetch Maximum Wait Timeout Unit a| Enumeration, one of:

** NANOSECONDS
** MICROSECONDS
** MILLISECONDS
** SECONDS
** MINUTES
** HOURS
** DAYS |  +++Determines the time unit for fetch maximum wait timeout scalar.+++ |  +++MILLISECONDS+++ | {nbsp}
| Principal a| String |  +++The entity that can be authenticated by a computer system or network. Principals can be individual people, computers, services, computational entities such as processes and threads, or any group of such things.+++ |  | *x*{nbsp}
| Service name a| String |  +++The Kerberos principal name that Kafka runs as.+++ |  | *x*{nbsp}
| Kerberos configuration file (krb5.conf) a| String |  +++The krb5.conf file contains Kerberos configuration information, including the locations of KDCs and admin servers for the Kerberos realms of interest, defaults for the current realm and for Kerberos applications, and mappings of hostnames onto Kerberos realms.+++ |  | {nbsp}
| Use ticket cache a| Boolean |  +++Set this to true, if you want the TGT to be obtained from the ticket cache. Set this option to false if you do not want to use the ticket cache. This connector will search for the ticket cache in the following locations: On Solaris and Linux it will look for the ticket cache in /tmp/krb5cc_uid where the uid is numeric user identifier. If the ticket cache is not available in the above location, or if we are on a Windows platform, it will look for the cache as {user.home}{file.separator}krb5cc_{user.name}. You can override the ticket cache location by using ticketCache. For Windows, if a ticket cannot be retrieved from the file ticket cache, it will use Local Security Authority (LSA) API to get the TGT.+++ |  +++false+++ | {nbsp}
| Ticket cache a| String |  +++Set this to the name of the ticket cache that contains the user's TGT. If this is set, useTicketCache must also be set to true; Otherwise a configuration error will be returned.+++ |  | {nbsp}
| Use keytab a| Boolean |  +++Set this to true if you want the module to get the principal's key from the the keytab. Default value is false. If keytab is not set then the module will locate the keytab from the Kerberos configuration file. If it is not specified in the Kerberos configuration file then it will look for the file {user.home}{file.separator}krb5.keytab.+++ |  +++false+++ | {nbsp}
| Keytab a| String |  +++Set this to the file name of the keytab to get the principal's secret key.+++ |  | {nbsp}
| Store key a| Boolean |  +++Set this to true if you want the principal's key to be stored in the Subject's private credentials.+++ |  +++false+++ | {nbsp}
| Reconnection a| <<Reconnection>> |  +++When the application is deployed, a connectivity test is performed on all connectors. If set to true, deployment will fail if the test doesn't pass after exhausting the associated reconnection strategy+++ |  | {nbsp}
|======================
[[consumer-config_consumer-sasl-plain-connection]]
===== Consumer SASL/PLAIN Connection


====== Parameters
[cols=".^20%,.^20%,.^35%,.^20%,^.^5%", options="header"]
|======================
| Name | Type | Description | Default Value | Required
| Bootstrap Server URLs a| Array of String |  +++The list of servers to bootstrap the connection with the kafka cluster. This can be a partial list of the available servers.+++ |  | *x*{nbsp}
| Endpoint identification algorithm a| String |  +++The endpoint identification algorithm used by clients to validate server host name. The default value is an empty string, which means it is disabled. Clients including client connections created by the broker for inter-broker communication verify that the broker host name matches the host name in the brokers certificate.+++ |  | {nbsp}
| Group ID a| String |  +++Default Group ID for all the Kafka Consumers that use this configuration.+++ |  | {nbsp}
| Consumer Amount a| Number |  +++Determines the number of consumers the connection will initially create.+++ |  +++1+++ | {nbsp}
| Maximum polling interval a| Number |  +++The configuration controls the maximum amount of time the client will wait for the response of a request. If the response is not received before the timeout elapses the client will resend the request if necessary or fail the request if retries are exhausted. This parameter can be overridden at source level.+++ |  +++60+++ | {nbsp}
| Maximum Polling Interval Time Unit a| Enumeration, one of:

** NANOSECONDS
** MICROSECONDS
** MILLISECONDS
** SECONDS
** MINUTES
** HOURS
** DAYS |  +++Determines the time unit for request timeout scalar. This parameter can be overridden at source level.+++ |  +++SECONDS+++ | {nbsp}
| Isolation Level a| Enumeration, one of:

** READ_UNCOMMITTED
** READ_COMMITTED |  +++Controls how to read messages written transactionally. If set to <code>read_committed</code>, consumer.poll() will only return" + " transactional messages which have been committed. If set to <code>read_uncommitted</code>' (the default), consumer.poll() will return all messages, even transactional messages" + " which have been aborted. Non-transactional messages will be returned unconditionally in either mode.</p> <p>Messages will always be returned in offset order. Hence, in " + " <code>read_committed</code> mode, consumer.poll() will only return messages up to the last stable offset (LSO), which is the one less than the offset of the first open transaction." + " In particular any messages appearing after messages belonging to ongoing transactions will be withheld until the relevant transaction has been completed. As a result, <code>read_committed</code>" + " consumers will not be able to read up to the high watermark when there are in flight transactions.</p><p> Further, when in <code>read_committed</code> the seekToEnd method will" + " return the LSO+++ |  +++READ_UNCOMMITTED+++ | {nbsp}
| Exclude internal topics a| Boolean |  +++Whether internal topics matching a subscribed pattern should be excluded from the subscription. It is always possible to explicitly subscribe to an internal topic.+++ |  +++true+++ | {nbsp}
| Auto offset reset a| Enumeration, one of:

** EARLIEST
** LATEST
** ERROR |  +++What to do when there is no initial offset in Kafka or if the current offset does not exist any more on the server (e.g. because that data has been deleted): EARLIEST: automatically reset the offset to the earliest offset. LATEST: automatically reset the offset to the latest offset. ERROR: throw error to the if no previous offset is found for the consumer's group.+++ |  +++LATEST+++ | {nbsp}
| Retry Backoff Timeout a| Number |  +++The amount of time to wait before attempting to retry a failed request to a given topic partition. This avoids repeatedly sending requests in a tight loop under some failure scenarios.+++ |  +++100+++ | {nbsp}
| Retry Backoff Timeout Time Unit a| Enumeration, one of:

** NANOSECONDS
** MICROSECONDS
** MILLISECONDS
** SECONDS
** MINUTES
** HOURS
** DAYS |  +++Determines the time unit for the reconnect backoff timeout scalar.+++ |  +++MILLISECONDS+++ | {nbsp}
| Check CRC a| Boolean |  +++Automatically check the CRC32 of the records consumed. This ensures no on-the-wire or on-disk corruption to the messages occurred. This check adds some overhead, so it may be disabled in cases seeking extreme performance.+++ |  +++true+++ | {nbsp}
| Default receive buffer size a| Number |  +++The size of the TCP receive buffer (SO_RCVBUF) to use when reading data. If the value is -1, the OS default will be used. This parameter can be overridden at source level.+++ |  +++64+++ | {nbsp}
| Default receive buffer size unit a| Enumeration, one of:

** BYTE
** KB
** MB
** GB |  +++The unit of measure for the receive buffer size scalar. This parameter can be overridden at source level.+++ |  +++KB+++ | {nbsp}
| Default send buffer size a| Number |  +++The size of the TCP send buffer (SO_SNDBUF) to use when sending data. If the value is -1, the OS default will be used. This parameter can be overridden at source level.+++ |  +++128+++ | {nbsp}
| Default send buffer size unit a| Enumeration, one of:

** BYTE
** KB
** MB
** GB |  +++The unit of measure for the send buffer size scalar. This parameter can be overridden at source level.+++ |  +++KB+++ | {nbsp}
| Request Timeout a| Number |  +++The configuration controls the maximum amount of time the client will wait for the response of a request. If the response is not received before the timeout elapses the client will resend the request if necessary or fail the request if retries are exhausted. This parameter can be overridden at source level.+++ |  +++30+++ | {nbsp}
| Request Timeout Time Unit a| Enumeration, one of:

** NANOSECONDS
** MICROSECONDS
** MILLISECONDS
** SECONDS
** MINUTES
** HOURS
** DAYS |  +++Determines the time unit for request timeout scalar. This parameter can be overridden at source level.+++ |  +++SECONDS+++ | {nbsp}
| Default record limit a| Number |  +++The maximum number of records returned on a poll call to the Kafka cluster. This parameter can be overridden at source level.+++ |  +++500+++ | {nbsp}
| DNS Lookups a| Enumeration, one of:

** DEFAULT
** USE_ALL_DNS_IPS
** RESOLVE_CANONICAL_BOOTSTRAP_SERVERS_ONLY |  +++Controls how the client uses DNS lookups. If set to use_all_dns_ips then, when the lookup returns multiple IP addresses for a hostname, they will all be attempted to connect to before failing the connection. Applies to both bootstrap and advertised servers. If the value is resolve_canonical_bootstrap_servers_only each entry will be resolved and expanded into a list of canonical names.+++ |  +++DEFAULT+++ | {nbsp}
| Heartbeat interval a| Number |  +++The expected time between heartbeats to the consumer coordinator when using Kafka's group management facilities. Heartbeats are used to ensure that the consumer's session stays active and to facilitate rebalancing when new consumers join or leave the group. The value must be set lower than session.timeout.ms, but typically should be set no higher than 1/3 of that value. It can be adjusted even lower to control the expected time for normal rebalances.+++ |  +++3+++ | {nbsp}
| Heartbeat Interval Time Unit a| Enumeration, one of:

** NANOSECONDS
** MICROSECONDS
** MILLISECONDS
** SECONDS
** MINUTES
** HOURS
** DAYS |  +++Determines the time unit for fetch heartbeat interval time scalar.+++ |  +++SECONDS+++ | {nbsp}
| Session timeout a| Number |  +++The timeout used to detect consumer failures when using Kafka's group management facility. The consumer sends periodic heartbeats to indicate its liveness to the broker. If no heartbeats are received by the broker before the expiration of this session timeout, then the broker will remove this consumer from the group and initiate a rebalance. Note that the value must be in the allowable range as configured in the broker configuration by group.min.session.timeout.ms and group.max.session.timeout.ms.+++ |  +++10+++ | {nbsp}
| Session timeout time unit a| Enumeration, one of:

** NANOSECONDS
** MICROSECONDS
** MILLISECONDS
** SECONDS
** MINUTES
** HOURS
** DAYS |  +++Determines the time unit for session timeout scalar.+++ |  +++SECONDS+++ | {nbsp}
| Connection maximum idle time a| Number |  +++Close idle connections after the number of milliseconds specified by this config.+++ |  +++540+++ | {nbsp}
| Connection maximum idle time time unit a| Enumeration, one of:

** NANOSECONDS
** MICROSECONDS
** MILLISECONDS
** SECONDS
** MINUTES
** HOURS
** DAYS |  +++Determines the time unit for connections maximum idle time scalar.+++ |  +++SECONDS+++ | {nbsp}
| TLS Configuration a| <<Tls>> |  +++Defines a TLS configuration, which can be used from both the client and server sides to secure communication for the Mule app. The connector will automatically set the 'security.protocol' to use for communication. Valid values are PLAINTEXT / SSL / SASL_PLAINTEXT / SASL_SSL. Default value when no configuration has been provided is PLAINTEXT(or SASL_PLAINTEXT for SASL authentication - kerberos/scram/plain). If SSL was configured as protocol on the broker side then the user needs to configure at least the keystore in the 'tls:context' child element of this config and the connector will automatically use SSL(or SASL_SSL for SASL authentication) as 'security.protocol'.+++ |  | {nbsp}
| Topic Subscription Patterns a| Array of String |  +++The list of subscription regular expressions to subscribe to. This topics will be automatically rebalanced between the amount of consumers of the topic.+++ |  | {nbsp}
| Assignments a| Array of <<TopicPartition>> |  +++The list of topic-partition pairs to assign. Note that there will be no automatic rebalance of the consumers +++ |  | {nbsp}
| Default fetch minimum size a| Number |  +++The minimum amount of data the server should return for a fetch request. If insufficient data is available the request will wait for that much data to accumulate before answering the request. The default setting of 1 byte means that fetch requests are answered as soon as a single byte of data is available or the fetch request times out waiting for data to arrive. Setting this to something greater than 1 will cause the server to wait for larger amounts of data to accumulate which can improve server throughput a bit at the cost of some additional latency. This parameter can be overridden at source level.+++ |  +++1+++ | {nbsp}
| Fetch Minimum Size Unit a| Enumeration, one of:

** BYTE
** KB
** MB
** GB |  |  +++BYTE+++ | {nbsp}
| Default fetch maximum size a| Number |  +++The maximum amount of data the server should return for a fetch request. Records are fetched in batches by the consumer, and if the first record batch in the first non-empty partition of the fetch is larger than this value, the record batch will still be returned to ensure that the consumer can make progress. As such, this is not an absolute maximum. The maximum record batch size accepted by the broker is defined via message.max.bytes (broker config) or max.message.bytes (topic config). Note that the consumer performs multiple fetches in parallel. This parameter can be overridden at source level.+++ |  +++1+++ | {nbsp}
| Default maximum fetch size unit a| Enumeration, one of:

** BYTE
** KB
** MB
** GB |  +++The unit of measure for the maximum partition fetch size scalar. This parameter can be overridden at source level.+++ |  +++MB+++ | {nbsp}
| Default maximum partition fetch size a| Number |  +++The maximum amount of data per-partition the server will return. Records are fetched in batches by the consumer. If the first record batch in the first non-empty partition of the fetch is larger than this limit, the batch will still be returned to ensure that the consumer can make progress. The maximum record batch size accepted by the broker is defined via message.max.bytes (broker config) or max.message.bytes (topic config). See fetch.max.bytes for limiting the consumer request size.This parameter can be overridden at source level.+++ |  +++1+++ | {nbsp}
| Default maximum partition fetch unit a| Enumeration, one of:

** BYTE
** KB
** MB
** GB |  +++The unit of measure for the maximum partition fetch size scalar. This parameter can be overridden at source level.+++ |  +++MB+++ | {nbsp}
| Fetch Maximum Wait Timeout a| Number |  +++The maximum amount of time the server will block before answering the fetch request if there isn't sufficient data to immediately satisfy the requirement given by fetch.min.bytes.+++ |  +++500+++ | {nbsp}
| Fetch Maximum Wait Timeout Unit a| Enumeration, one of:

** NANOSECONDS
** MICROSECONDS
** MILLISECONDS
** SECONDS
** MINUTES
** HOURS
** DAYS |  +++Determines the time unit for fetch maximum wait timeout scalar.+++ |  +++MILLISECONDS+++ | {nbsp}
| Username a| String |  +++The username with which to login.+++ |  | *x*{nbsp}
| Password a| String |  +++The password with which to login.+++ |  | *x*{nbsp}
| Reconnection a| <<Reconnection>> |  +++When the application is deployed, a connectivity test is performed on all connectors. If set to true, deployment will fail if the test doesn't pass after exhausting the associated reconnection strategy+++ |  | {nbsp}
|======================
[[consumer-config_consumer-sasl-scram-connection]]
===== Consumer SASL/SCRAM Connection

+++
Salted Challenge Response Authentication Mechanism (SCRAM), or SASL/SCRAM, is a family of SASL mechanisms that addresses the security concerns with traditional mechanisms that perform username/password authentication like PLAIN. Apache KafkaÂ® supports SCRAM-SHA-256 and SCRAM-SHA-512.
+++

====== Parameters
[cols=".^20%,.^20%,.^35%,.^20%,^.^5%", options="header"]
|======================
| Name | Type | Description | Default Value | Required
| Bootstrap Server URLs a| Array of String |  +++The list of servers to bootstrap the connection with the kafka cluster. This can be a partial list of the available servers.+++ |  | *x*{nbsp}
| Endpoint identification algorithm a| String |  +++The endpoint identification algorithm used by clients to validate server host name. The default value is an empty string, which means it is disabled. Clients including client connections created by the broker for inter-broker communication verify that the broker host name matches the host name in the brokers certificate.+++ |  | {nbsp}
| Group ID a| String |  +++Default Group ID for all the Kafka Consumers that use this configuration.+++ |  | {nbsp}
| Consumer Amount a| Number |  +++Determines the number of consumers the connection will initially create.+++ |  +++1+++ | {nbsp}
| Maximum polling interval a| Number |  +++The configuration controls the maximum amount of time the client will wait for the response of a request. If the response is not received before the timeout elapses the client will resend the request if necessary or fail the request if retries are exhausted. This parameter can be overridden at source level.+++ |  +++60+++ | {nbsp}
| Maximum Polling Interval Time Unit a| Enumeration, one of:

** NANOSECONDS
** MICROSECONDS
** MILLISECONDS
** SECONDS
** MINUTES
** HOURS
** DAYS |  +++Determines the time unit for request timeout scalar. This parameter can be overridden at source level.+++ |  +++SECONDS+++ | {nbsp}
| Isolation Level a| Enumeration, one of:

** READ_UNCOMMITTED
** READ_COMMITTED |  +++Controls how to read messages written transactionally. If set to <code>read_committed</code>, consumer.poll() will only return" + " transactional messages which have been committed. If set to <code>read_uncommitted</code>' (the default), consumer.poll() will return all messages, even transactional messages" + " which have been aborted. Non-transactional messages will be returned unconditionally in either mode.</p> <p>Messages will always be returned in offset order. Hence, in " + " <code>read_committed</code> mode, consumer.poll() will only return messages up to the last stable offset (LSO), which is the one less than the offset of the first open transaction." + " In particular any messages appearing after messages belonging to ongoing transactions will be withheld until the relevant transaction has been completed. As a result, <code>read_committed</code>" + " consumers will not be able to read up to the high watermark when there are in flight transactions.</p><p> Further, when in <code>read_committed</code> the seekToEnd method will" + " return the LSO+++ |  +++READ_UNCOMMITTED+++ | {nbsp}
| Exclude internal topics a| Boolean |  +++Whether internal topics matching a subscribed pattern should be excluded from the subscription. It is always possible to explicitly subscribe to an internal topic.+++ |  +++true+++ | {nbsp}
| Auto offset reset a| Enumeration, one of:

** EARLIEST
** LATEST
** ERROR |  +++What to do when there is no initial offset in Kafka or if the current offset does not exist any more on the server (e.g. because that data has been deleted): EARLIEST: automatically reset the offset to the earliest offset. LATEST: automatically reset the offset to the latest offset. ERROR: throw error to the if no previous offset is found for the consumer's group.+++ |  +++LATEST+++ | {nbsp}
| Retry Backoff Timeout a| Number |  +++The amount of time to wait before attempting to retry a failed request to a given topic partition. This avoids repeatedly sending requests in a tight loop under some failure scenarios.+++ |  +++100+++ | {nbsp}
| Retry Backoff Timeout Time Unit a| Enumeration, one of:

** NANOSECONDS
** MICROSECONDS
** MILLISECONDS
** SECONDS
** MINUTES
** HOURS
** DAYS |  +++Determines the time unit for the reconnect backoff timeout scalar.+++ |  +++MILLISECONDS+++ | {nbsp}
| Check CRC a| Boolean |  +++Automatically check the CRC32 of the records consumed. This ensures no on-the-wire or on-disk corruption to the messages occurred. This check adds some overhead, so it may be disabled in cases seeking extreme performance.+++ |  +++true+++ | {nbsp}
| Default receive buffer size a| Number |  +++The size of the TCP receive buffer (SO_RCVBUF) to use when reading data. If the value is -1, the OS default will be used. This parameter can be overridden at source level.+++ |  +++64+++ | {nbsp}
| Default receive buffer size unit a| Enumeration, one of:

** BYTE
** KB
** MB
** GB |  +++The unit of measure for the receive buffer size scalar. This parameter can be overridden at source level.+++ |  +++KB+++ | {nbsp}
| Default send buffer size a| Number |  +++The size of the TCP send buffer (SO_SNDBUF) to use when sending data. If the value is -1, the OS default will be used. This parameter can be overridden at source level.+++ |  +++128+++ | {nbsp}
| Default send buffer size unit a| Enumeration, one of:

** BYTE
** KB
** MB
** GB |  +++The unit of measure for the send buffer size scalar. This parameter can be overridden at source level.+++ |  +++KB+++ | {nbsp}
| Request Timeout a| Number |  +++The configuration controls the maximum amount of time the client will wait for the response of a request. If the response is not received before the timeout elapses the client will resend the request if necessary or fail the request if retries are exhausted. This parameter can be overridden at source level.+++ |  +++30+++ | {nbsp}
| Request Timeout Time Unit a| Enumeration, one of:

** NANOSECONDS
** MICROSECONDS
** MILLISECONDS
** SECONDS
** MINUTES
** HOURS
** DAYS |  +++Determines the time unit for request timeout scalar. This parameter can be overridden at source level.+++ |  +++SECONDS+++ | {nbsp}
| Default record limit a| Number |  +++The maximum number of records returned on a poll call to the Kafka cluster. This parameter can be overridden at source level.+++ |  +++500+++ | {nbsp}
| DNS Lookups a| Enumeration, one of:

** DEFAULT
** USE_ALL_DNS_IPS
** RESOLVE_CANONICAL_BOOTSTRAP_SERVERS_ONLY |  +++Controls how the client uses DNS lookups. If set to use_all_dns_ips then, when the lookup returns multiple IP addresses for a hostname, they will all be attempted to connect to before failing the connection. Applies to both bootstrap and advertised servers. If the value is resolve_canonical_bootstrap_servers_only each entry will be resolved and expanded into a list of canonical names.+++ |  +++DEFAULT+++ | {nbsp}
| Heartbeat interval a| Number |  +++The expected time between heartbeats to the consumer coordinator when using Kafka's group management facilities. Heartbeats are used to ensure that the consumer's session stays active and to facilitate rebalancing when new consumers join or leave the group. The value must be set lower than session.timeout.ms, but typically should be set no higher than 1/3 of that value. It can be adjusted even lower to control the expected time for normal rebalances.+++ |  +++3+++ | {nbsp}
| Heartbeat Interval Time Unit a| Enumeration, one of:

** NANOSECONDS
** MICROSECONDS
** MILLISECONDS
** SECONDS
** MINUTES
** HOURS
** DAYS |  +++Determines the time unit for fetch heartbeat interval time scalar.+++ |  +++SECONDS+++ | {nbsp}
| Session timeout a| Number |  +++The timeout used to detect consumer failures when using Kafka's group management facility. The consumer sends periodic heartbeats to indicate its liveness to the broker. If no heartbeats are received by the broker before the expiration of this session timeout, then the broker will remove this consumer from the group and initiate a rebalance. Note that the value must be in the allowable range as configured in the broker configuration by group.min.session.timeout.ms and group.max.session.timeout.ms.+++ |  +++10+++ | {nbsp}
| Session timeout time unit a| Enumeration, one of:

** NANOSECONDS
** MICROSECONDS
** MILLISECONDS
** SECONDS
** MINUTES
** HOURS
** DAYS |  +++Determines the time unit for session timeout scalar.+++ |  +++SECONDS+++ | {nbsp}
| Connection maximum idle time a| Number |  +++Close idle connections after the number of milliseconds specified by this config.+++ |  +++540+++ | {nbsp}
| Connection maximum idle time time unit a| Enumeration, one of:

** NANOSECONDS
** MICROSECONDS
** MILLISECONDS
** SECONDS
** MINUTES
** HOURS
** DAYS |  +++Determines the time unit for connections maximum idle time scalar.+++ |  +++SECONDS+++ | {nbsp}
| TLS Configuration a| <<Tls>> |  +++Defines a TLS configuration, which can be used from both the client and server sides to secure communication for the Mule app. The connector will automatically set the 'security.protocol' to use for communication. Valid values are PLAINTEXT / SSL / SASL_PLAINTEXT / SASL_SSL. Default value when no configuration has been provided is PLAINTEXT(or SASL_PLAINTEXT for SASL authentication - kerberos/scram/plain). If SSL was configured as protocol on the broker side then the user needs to configure at least the keystore in the 'tls:context' child element of this config and the connector will automatically use SSL(or SASL_SSL for SASL authentication) as 'security.protocol'.+++ |  | {nbsp}
| Topic Subscription Patterns a| Array of String |  +++The list of subscription regular expressions to subscribe to. This topics will be automatically rebalanced between the amount of consumers of the topic.+++ |  | {nbsp}
| Assignments a| Array of <<TopicPartition>> |  +++The list of topic-partition pairs to assign. Note that there will be no automatic rebalance of the consumers +++ |  | {nbsp}
| Default fetch minimum size a| Number |  +++The minimum amount of data the server should return for a fetch request. If insufficient data is available the request will wait for that much data to accumulate before answering the request. The default setting of 1 byte means that fetch requests are answered as soon as a single byte of data is available or the fetch request times out waiting for data to arrive. Setting this to something greater than 1 will cause the server to wait for larger amounts of data to accumulate which can improve server throughput a bit at the cost of some additional latency. This parameter can be overridden at source level.+++ |  +++1+++ | {nbsp}
| Fetch Minimum Size Unit a| Enumeration, one of:

** BYTE
** KB
** MB
** GB |  |  +++BYTE+++ | {nbsp}
| Default fetch maximum size a| Number |  +++The maximum amount of data the server should return for a fetch request. Records are fetched in batches by the consumer, and if the first record batch in the first non-empty partition of the fetch is larger than this value, the record batch will still be returned to ensure that the consumer can make progress. As such, this is not an absolute maximum. The maximum record batch size accepted by the broker is defined via message.max.bytes (broker config) or max.message.bytes (topic config). Note that the consumer performs multiple fetches in parallel. This parameter can be overridden at source level.+++ |  +++1+++ | {nbsp}
| Default maximum fetch size unit a| Enumeration, one of:

** BYTE
** KB
** MB
** GB |  +++The unit of measure for the maximum partition fetch size scalar. This parameter can be overridden at source level.+++ |  +++MB+++ | {nbsp}
| Default maximum partition fetch size a| Number |  +++The maximum amount of data per-partition the server will return. Records are fetched in batches by the consumer. If the first record batch in the first non-empty partition of the fetch is larger than this limit, the batch will still be returned to ensure that the consumer can make progress. The maximum record batch size accepted by the broker is defined via message.max.bytes (broker config) or max.message.bytes (topic config). See fetch.max.bytes for limiting the consumer request size.This parameter can be overridden at source level.+++ |  +++1+++ | {nbsp}
| Default maximum partition fetch unit a| Enumeration, one of:

** BYTE
** KB
** MB
** GB |  +++The unit of measure for the maximum partition fetch size scalar. This parameter can be overridden at source level.+++ |  +++MB+++ | {nbsp}
| Fetch Maximum Wait Timeout a| Number |  +++The maximum amount of time the server will block before answering the fetch request if there isn't sufficient data to immediately satisfy the requirement given by fetch.min.bytes.+++ |  +++500+++ | {nbsp}
| Fetch Maximum Wait Timeout Unit a| Enumeration, one of:

** NANOSECONDS
** MICROSECONDS
** MILLISECONDS
** SECONDS
** MINUTES
** HOURS
** DAYS |  +++Determines the time unit for fetch maximum wait timeout scalar.+++ |  +++MILLISECONDS+++ | {nbsp}
| Username a| String |  +++The username with which to login.+++ |  | *x*{nbsp}
| Password a| String |  +++The password with which to login.+++ |  | *x*{nbsp}
| Encryption type a| Enumeration, one of:

** SCRAM_SHA_256
** SCRAM_SHA_512 |  +++The encryption algorithm used by SCRAM. Only acceptable values are SCRAM_SHA_256 and SCRAM_SHA_512.+++ |  | *x*{nbsp}
| Reconnection a| <<Reconnection>> |  +++When the application is deployed, a connectivity test is performed on all connectors. If set to true, deployment will fail if the test doesn't pass after exhausting the associated reconnection strategy+++ |  | {nbsp}
|======================

==== Associated Operations
* <<commit>> {nbsp}
* <<consume>> {nbsp}
* <<seek>> {nbsp}

==== Associated Sources
* <<batch-message-listener>> {nbsp}
* <<message-listener>> {nbsp}

---
[[producer-config]]
=== Producer configuration


==== Parameters
[cols=".^20%,.^20%,.^35%,.^20%,^.^5%", options="header"]
|======================
| Name | Type | Description | Default Value | Required
|Name | String | The name for this configuration. Connectors reference the configuration with this name. | | *x*{nbsp}
| Connection a| * <<producer-config_producer-plaintext-connection, Producer Plaintext Connection>> {nbsp}
* <<producer-config_producer-sasl-gssapi-connection, Producer Kerberos Connection>> {nbsp}
* <<producer-config_producer-sasl-plain-connection, Producer SASL/PLAIN Connection>> {nbsp}
* <<producer-config_producer-sasl-scram-connection, Producer SASL/SCRAM Connection>> {nbsp}
 | The connection types that can be provided to this configuration. | | *x*{nbsp}
| Default topic a| String |  +++A default topic name to be used by the producer operations, overridable at operation config level.+++ |  +++defaultTopicName+++ | {nbsp}
| Zone ID a| String |  +++The time-zone ID that is going to be used when returning date or timestamp information of a determined message. Defaults to the system default.+++ |  | {nbsp}
| Expiration Policy a| <<ExpirationPolicy>> |  +++Configures the minimum amount of time that a dynamic configuration instance can remain idle before the runtime considers it eligible for expiration. This does not mean that the platform will expire the instance at the exact moment that it becomes eligible. The runtime will actually purge the instances when it sees it fit.+++ |  | {nbsp}
|======================

==== Connection Types
[[producer-config_producer-plaintext-connection]]
===== Producer Plaintext Connection


====== Parameters
[cols=".^20%,.^20%,.^35%,.^20%,^.^5%", options="header"]
|======================
| Name | Type | Description | Default Value | Required
| Bootstrap Server URLs a| Array of String |  +++The list of servers to bootstrap the connection with the kafka cluster. This can be a partial list of the available servers.+++ |  | *x*{nbsp}
| Endpoint identification algorithm a| String |  +++The endpoint identification algorithm used by clients to validate server host name. The default value is an empty string, which means it is disabled. Clients including client connections created by the broker for inter-broker communication verify that the broker host name matches the host name in the brokers certificate.+++ |  | {nbsp}
| Batch size a| Number |  +++The producer will attempt to batch records together into fewer requests whenever multiple records are being sent to the same partition. This helps performance on both the client and the server. This configuration controls the default batch size in bytes. No attempt will be made to batch records larger than this size. Requests sent to brokers will contain multiple batches, one for each partition with data available to be sent. A small batch size will make batching less common and may reduce throughput (a batch size of zero will disable batching entirely). A very large batch size may use memory a bit more wastefully as we will always allocate a buffer of the specified batch size in anticipation of additional records.+++ |  +++16+++ | {nbsp}
| The batch size unit of measure. a| Enumeration, one of:

** BYTE
** KB
** MB
** GB |  +++The unit of measure for the batch size scalar.+++ |  +++KB+++ | {nbsp}
| Buffer size a| Number |  +++The total bytes of memory the producer can use to buffer records waiting to be sent to the server. If records are sent faster than they can be delivered to the server the producer will block for max.block.ms after which it will throw an exception. This setting should correspond roughly to the total memory the producer will use, but is not a hard bound since not all memory the producer uses is used for buffering. Some additional memory will be used for compression (if compression is enabled) as well as for maintaining in-flight requests. The default value in the Kafka docs is of 33554432 (32MB), but this should be capped to align with expected values for mule instances in cloudhub (v0.1 core)+++ |  +++1000+++ | {nbsp}
| The buffer memory size unit of measure. a| Enumeration, one of:

** BYTE
** KB
** MB
** GB |  +++The unit of measure for the max request size scalar.+++ |  +++KB+++ | {nbsp}
| DNS lookups a| Enumeration, one of:

** DEFAULT
** USE_ALL_DNS_IPS
** RESOLVE_CANONICAL_BOOTSTRAP_SERVERS_ONLY |  +++Controls how the client uses DNS lookups. If set to use_all_dns_ips then, when the lookup returns multiple IP addresses for a hostname, they will all be attempted to connect to before failing the connection. Applies to both bootstrap and advertised servers. If the value is resolve_canonical_bootstrap_servers_only each entry will be resolved and expanded into a list of canonical names.+++ |  +++DEFAULT+++ | {nbsp}
| Compression type a| Enumeration, one of:

** NONE
** GZIP
** SNAPPY
** LZ4
** ZSTD |  +++The compression type for all data generated by the producer. The default is none (i.e. no compression). Valid values are none, gzip, snappy, lz4, or zstd. Compression is of full batches of data, so the efficacy of batching will also impact the compression ratio (more batching means better compression).+++ |  +++NONE+++ | {nbsp}
| Connections maximum idle time a| Number |  +++Close idle connections after the value specified by this config.+++ |  +++540+++ | {nbsp}
| Connections maximum idle time unit a| Enumeration, one of:

** NANOSECONDS
** MICROSECONDS
** MILLISECONDS
** SECONDS
** MINUTES
** HOURS
** DAYS |  +++Determines the time unit for the connections maximum idle scalar.+++ |  +++SECONDS+++ | {nbsp}
| Delivery timeout a| Number |  +++An upper bound on the time to report success or failure after a call to send() returns. This limits the total time that a record will be delayed prior to sending, the time to await acknowledgement from the broker (if expected), and the time allowed for retriable send failures. The producer may report failure to send a record earlier than this config if either an unrecoverable error is encountered, the retries have been exhausted, or the record is added to a batch which reached an earlier delivery expiration deadline. The value of this config should be greater than or equal to the sum of request.timeout.ms and linger.ms.+++ |  +++120+++ | {nbsp}
| Delivery Timeout Time Unit a| Enumeration, one of:

** NANOSECONDS
** MICROSECONDS
** MILLISECONDS
** SECONDS
** MINUTES
** HOURS
** DAYS |  +++Determines the time unit for the delivery timeout scalar.+++ |  +++SECONDS+++ | {nbsp}
| Enable idempotence a| Boolean |  +++When set to 'true', the producer will ensure that exactly one copy of each message is written in the stream. If 'false', producer retries due to broker failures, etc, may write duplicates of the retried message in the stream. Note that enabling idempotence requires max.in.flight.requests.per.connection to be less than or equal to 5, retries to be greater than 0 and acks must be 'all'. If these values are not explicitly set by the user, suitable values will be chosen. If incompatible values are set, a ConnectionException will be thrown+++ |  +++false+++ | {nbsp}
| Linger time  a| Number |  +++The producer groups together any records that arrive in between request transmissions into a single batched request. Normally this occurs only under load when records arrive faster than they can be sent out. However in some circumstances the client may want to reduce the number of requests even under moderate load. This setting accomplishes this by adding a small amount of artificial delay?that is, rather than immediately sending out a record the producer will wait for up to the given delay to allow other records to be sent so that the sends can be batched together. This can be thought of as analogous to Nagle's algorithm in TCP. This setting gives the upper bound on the delay for batching: once we get batch.size worth of records for a partition it will be sent immediately regardless of this setting, however if we have fewer than this many bytes accumulated for this partition we will 'linger' for the specified time waiting for more records to show up. This setting defaults to 0 (i.e. no delay). Setting linger.ms=5, for example, would have the effect of reducing the number of requests sent but would add up to 5ms of latency to records sent in the absence of load.+++ |  +++0+++ | {nbsp}
| Linger Time Unit a| Enumeration, one of:

** NANOSECONDS
** MICROSECONDS
** MILLISECONDS
** SECONDS
** MINUTES
** HOURS
** DAYS |  +++Determines the time unit for the linger time scalar.+++ |  +++SECONDS+++ | {nbsp}
| Maximum block time  a| Number |  +++The configuration controls how long KafkaProducer.send() and KafkaProducer.partitionsFor() will block.These methods can be blocked either because the buffer is full or metadata unavailable.Blocking in the user-supplied serializers or partitioner will not be counted against this timeout.+++ |  +++60+++ | {nbsp}
| Maximum block time unit a| Enumeration, one of:

** NANOSECONDS
** MICROSECONDS
** MILLISECONDS
** SECONDS
** MINUTES
** HOURS
** DAYS |  +++Determines the time unit for the maximum block time scalar.+++ |  +++SECONDS+++ | {nbsp}
| Maximum in flight requests a| Number |  +++The maximum number of unacknowledged requests the client will send on a single connection before blocking. Note that if this setting is set to be greater than 1 and there are failed sends, there is a risk of message re-ordering due to retries (i.e., if retries are enabled).+++ |  +++5+++ | {nbsp}
| Maximum request size a| Number |  +++The maximum size of a request in bytes. This setting will limit the number of record batches the producer will send in a single request to avoid sending huge requests. This is also effectively a cap on the maximum record batch size. Note that the server has its own cap on record batch size which may be different from this.+++ |  +++1+++ | {nbsp}
| Maximum request size unit. a| Enumeration, one of:

** BYTE
** KB
** MB
** GB |  +++The unit of measure for the max request size scalar.+++ |  +++MB+++ | {nbsp}
| Producer acknowledge mode a| Enumeration, one of:

** NONE
** LEADER_ONLY
** ALL |  +++The number of acknowledgments the producer requires the leader to have received before considering a request complete. This controls the durability of records that are sent+++ |  +++NONE+++ | {nbsp}
| Default receive buffer size a| Number |  +++The size of the TCP receive buffer (SO_RCVBUF) to use when reading data. If the value is -1, the OS default will be used. This parameter can be overridden at source level.+++ |  +++64+++ | {nbsp}
| Default receive buffer size unit a| Enumeration, one of:

** BYTE
** KB
** MB
** GB |  +++The unit of measure for the receive buffer size scalar. This parameter can be overridden at source level.+++ |  +++KB+++ | {nbsp}
| Retries amount a| Number |  +++Setting a value greater than zero will cause the client to resend any record whose send fails with a potentially transient error. Note that this retry is no different than if the client resent the record upon receiving the error. Allowing retries without setting max.in.flight.requests.per.connection to 1 will potentially change the ordering of records because if two batches are sent to a single partition, and the first fails and is retried but the second succeeds, then the records in the second batch may appear first. Note additionally that produce requests will be failed before the number of retries has been exhausted if the timeout configured by delivery.timeout.ms expires first before successful acknowledgement. Users should generally prefer to leave this config unset and instead use delivery.timeout.ms to control retry behavior.+++ |  +++1+++ | {nbsp}
| Retry Backoff Timeout Time Unit a| Enumeration, one of:

** NANOSECONDS
** MICROSECONDS
** MILLISECONDS
** SECONDS
** MINUTES
** HOURS
** DAYS |  +++Determines the time unit for the retry backoff timeout time scalar.+++ |  +++MILLISECONDS+++ | {nbsp}
| Retry backoff timeout a| Number |  +++The amount of time to wait before attempting to retry a failed request to a given topic partition. This avoids repeatedly sending requests in a tight loop under some failure scenarios.+++ |  +++100+++ | {nbsp}
| Default send buffer size a| Number |  +++The size of the TCP send buffer (SO_SNDBUF) to use when sending data. If the value is -1, the OS default will be used. This parameter can be overridden at source level.+++ |  +++128+++ | {nbsp}
| Default send buffer size unit a| Enumeration, one of:

** BYTE
** KB
** MB
** GB |  +++The unit of measure for the send buffer size scalar. This parameter can be overridden at source level.+++ |  +++KB+++ | {nbsp}
| Default request timeout time unit a| Enumeration, one of:

** NANOSECONDS
** MICROSECONDS
** MILLISECONDS
** SECONDS
** MINUTES
** HOURS
** DAYS |  +++Determines the time unit for the request timeout time scalar.+++ |  +++SECONDS+++ | {nbsp}
| Default request timeout a| Number |  +++The configuration controls the maximum amount of time the client will wait for the response of a request. If the response is not received before the timeout elapses the client will resend the request if necessary or fail the request if retries are exhausted. This should be larger than replica.lag.time.max.ms (a broker configuration) to reduce the possibility of message duplication due to unnecessary producer retries.+++ |  +++30+++ | {nbsp}
| TLS Configuration a| <<Tls>> |  +++Defines a TLS configuration, which can be used from both the client and server sides to secure communication for the Mule app. The connector will automatically set the 'security.protocol' to use for communication. Valid values are PLAINTEXT / SSL / SASL_PLAINTEXT / SASL_SSL. Default value when no configuration has been provided is PLAINTEXT(or SASL_PLAINTEXT for SASL authentication - kerberos/scram/plain). If SSL was configured as protocol on the broker side then the user needs to configure at least the keystore in the 'tls:context' child element of this config and the connector will automatically use SSL(or SASL_SSL for SASL authentication) as 'security.protocol'.+++ |  | {nbsp}
| Reconnection a| <<Reconnection>> |  +++When the application is deployed, a connectivity test is performed on all connectors. If set to true, deployment will fail if the test doesn't pass after exhausting the associated reconnection strategy+++ |  | {nbsp}
|======================
[[producer-config_producer-sasl-gssapi-connection]]
===== Producer Kerberos Connection


====== Parameters
[cols=".^20%,.^20%,.^35%,.^20%,^.^5%", options="header"]
|======================
| Name | Type | Description | Default Value | Required
| Bootstrap Server URLs a| Array of String |  +++The list of servers to bootstrap the connection with the kafka cluster. This can be a partial list of the available servers.+++ |  | *x*{nbsp}
| Endpoint identification algorithm a| String |  +++The endpoint identification algorithm used by clients to validate server host name. The default value is an empty string, which means it is disabled. Clients including client connections created by the broker for inter-broker communication verify that the broker host name matches the host name in the brokers certificate.+++ |  | {nbsp}
| Batch size a| Number |  +++The producer will attempt to batch records together into fewer requests whenever multiple records are being sent to the same partition. This helps performance on both the client and the server. This configuration controls the default batch size in bytes. No attempt will be made to batch records larger than this size. Requests sent to brokers will contain multiple batches, one for each partition with data available to be sent. A small batch size will make batching less common and may reduce throughput (a batch size of zero will disable batching entirely). A very large batch size may use memory a bit more wastefully as we will always allocate a buffer of the specified batch size in anticipation of additional records.+++ |  +++16+++ | {nbsp}
| The batch size unit of measure. a| Enumeration, one of:

** BYTE
** KB
** MB
** GB |  +++The unit of measure for the batch size scalar.+++ |  +++KB+++ | {nbsp}
| Buffer size a| Number |  +++The total bytes of memory the producer can use to buffer records waiting to be sent to the server. If records are sent faster than they can be delivered to the server the producer will block for max.block.ms after which it will throw an exception. This setting should correspond roughly to the total memory the producer will use, but is not a hard bound since not all memory the producer uses is used for buffering. Some additional memory will be used for compression (if compression is enabled) as well as for maintaining in-flight requests. The default value in the Kafka docs is of 33554432 (32MB), but this should be capped to align with expected values for mule instances in cloudhub (v0.1 core)+++ |  +++1000+++ | {nbsp}
| The buffer memory size unit of measure. a| Enumeration, one of:

** BYTE
** KB
** MB
** GB |  +++The unit of measure for the max request size scalar.+++ |  +++KB+++ | {nbsp}
| DNS lookups a| Enumeration, one of:

** DEFAULT
** USE_ALL_DNS_IPS
** RESOLVE_CANONICAL_BOOTSTRAP_SERVERS_ONLY |  +++Controls how the client uses DNS lookups. If set to use_all_dns_ips then, when the lookup returns multiple IP addresses for a hostname, they will all be attempted to connect to before failing the connection. Applies to both bootstrap and advertised servers. If the value is resolve_canonical_bootstrap_servers_only each entry will be resolved and expanded into a list of canonical names.+++ |  +++DEFAULT+++ | {nbsp}
| Compression type a| Enumeration, one of:

** NONE
** GZIP
** SNAPPY
** LZ4
** ZSTD |  +++The compression type for all data generated by the producer. The default is none (i.e. no compression). Valid values are none, gzip, snappy, lz4, or zstd. Compression is of full batches of data, so the efficacy of batching will also impact the compression ratio (more batching means better compression).+++ |  +++NONE+++ | {nbsp}
| Connections maximum idle time a| Number |  +++Close idle connections after the value specified by this config.+++ |  +++540+++ | {nbsp}
| Connections maximum idle time unit a| Enumeration, one of:

** NANOSECONDS
** MICROSECONDS
** MILLISECONDS
** SECONDS
** MINUTES
** HOURS
** DAYS |  +++Determines the time unit for the connections maximum idle scalar.+++ |  +++SECONDS+++ | {nbsp}
| Delivery timeout a| Number |  +++An upper bound on the time to report success or failure after a call to send() returns. This limits the total time that a record will be delayed prior to sending, the time to await acknowledgement from the broker (if expected), and the time allowed for retriable send failures. The producer may report failure to send a record earlier than this config if either an unrecoverable error is encountered, the retries have been exhausted, or the record is added to a batch which reached an earlier delivery expiration deadline. The value of this config should be greater than or equal to the sum of request.timeout.ms and linger.ms.+++ |  +++120+++ | {nbsp}
| Delivery Timeout Time Unit a| Enumeration, one of:

** NANOSECONDS
** MICROSECONDS
** MILLISECONDS
** SECONDS
** MINUTES
** HOURS
** DAYS |  +++Determines the time unit for the delivery timeout scalar.+++ |  +++SECONDS+++ | {nbsp}
| Enable idempotence a| Boolean |  +++When set to 'true', the producer will ensure that exactly one copy of each message is written in the stream. If 'false', producer retries due to broker failures, etc, may write duplicates of the retried message in the stream. Note that enabling idempotence requires max.in.flight.requests.per.connection to be less than or equal to 5, retries to be greater than 0 and acks must be 'all'. If these values are not explicitly set by the user, suitable values will be chosen. If incompatible values are set, a ConnectionException will be thrown+++ |  +++false+++ | {nbsp}
| Linger time  a| Number |  +++The producer groups together any records that arrive in between request transmissions into a single batched request. Normally this occurs only under load when records arrive faster than they can be sent out. However in some circumstances the client may want to reduce the number of requests even under moderate load. This setting accomplishes this by adding a small amount of artificial delay?that is, rather than immediately sending out a record the producer will wait for up to the given delay to allow other records to be sent so that the sends can be batched together. This can be thought of as analogous to Nagle's algorithm in TCP. This setting gives the upper bound on the delay for batching: once we get batch.size worth of records for a partition it will be sent immediately regardless of this setting, however if we have fewer than this many bytes accumulated for this partition we will 'linger' for the specified time waiting for more records to show up. This setting defaults to 0 (i.e. no delay). Setting linger.ms=5, for example, would have the effect of reducing the number of requests sent but would add up to 5ms of latency to records sent in the absence of load.+++ |  +++0+++ | {nbsp}
| Linger Time Unit a| Enumeration, one of:

** NANOSECONDS
** MICROSECONDS
** MILLISECONDS
** SECONDS
** MINUTES
** HOURS
** DAYS |  +++Determines the time unit for the linger time scalar.+++ |  +++SECONDS+++ | {nbsp}
| Maximum block time  a| Number |  +++The configuration controls how long KafkaProducer.send() and KafkaProducer.partitionsFor() will block.These methods can be blocked either because the buffer is full or metadata unavailable.Blocking in the user-supplied serializers or partitioner will not be counted against this timeout.+++ |  +++60+++ | {nbsp}
| Maximum block time unit a| Enumeration, one of:

** NANOSECONDS
** MICROSECONDS
** MILLISECONDS
** SECONDS
** MINUTES
** HOURS
** DAYS |  +++Determines the time unit for the maximum block time scalar.+++ |  +++SECONDS+++ | {nbsp}
| Maximum in flight requests a| Number |  +++The maximum number of unacknowledged requests the client will send on a single connection before blocking. Note that if this setting is set to be greater than 1 and there are failed sends, there is a risk of message re-ordering due to retries (i.e., if retries are enabled).+++ |  +++5+++ | {nbsp}
| Maximum request size a| Number |  +++The maximum size of a request in bytes. This setting will limit the number of record batches the producer will send in a single request to avoid sending huge requests. This is also effectively a cap on the maximum record batch size. Note that the server has its own cap on record batch size which may be different from this.+++ |  +++1+++ | {nbsp}
| Maximum request size unit. a| Enumeration, one of:

** BYTE
** KB
** MB
** GB |  +++The unit of measure for the max request size scalar.+++ |  +++MB+++ | {nbsp}
| Producer acknowledge mode a| Enumeration, one of:

** NONE
** LEADER_ONLY
** ALL |  +++The number of acknowledgments the producer requires the leader to have received before considering a request complete. This controls the durability of records that are sent+++ |  +++NONE+++ | {nbsp}
| Default receive buffer size a| Number |  +++The size of the TCP receive buffer (SO_RCVBUF) to use when reading data. If the value is -1, the OS default will be used. This parameter can be overridden at source level.+++ |  +++64+++ | {nbsp}
| Default receive buffer size unit a| Enumeration, one of:

** BYTE
** KB
** MB
** GB |  +++The unit of measure for the receive buffer size scalar. This parameter can be overridden at source level.+++ |  +++KB+++ | {nbsp}
| Retries amount a| Number |  +++Setting a value greater than zero will cause the client to resend any record whose send fails with a potentially transient error. Note that this retry is no different than if the client resent the record upon receiving the error. Allowing retries without setting max.in.flight.requests.per.connection to 1 will potentially change the ordering of records because if two batches are sent to a single partition, and the first fails and is retried but the second succeeds, then the records in the second batch may appear first. Note additionally that produce requests will be failed before the number of retries has been exhausted if the timeout configured by delivery.timeout.ms expires first before successful acknowledgement. Users should generally prefer to leave this config unset and instead use delivery.timeout.ms to control retry behavior.+++ |  +++1+++ | {nbsp}
| Retry Backoff Timeout Time Unit a| Enumeration, one of:

** NANOSECONDS
** MICROSECONDS
** MILLISECONDS
** SECONDS
** MINUTES
** HOURS
** DAYS |  +++Determines the time unit for the retry backoff timeout time scalar.+++ |  +++MILLISECONDS+++ | {nbsp}
| Retry backoff timeout a| Number |  +++The amount of time to wait before attempting to retry a failed request to a given topic partition. This avoids repeatedly sending requests in a tight loop under some failure scenarios.+++ |  +++100+++ | {nbsp}
| Default send buffer size a| Number |  +++The size of the TCP send buffer (SO_SNDBUF) to use when sending data. If the value is -1, the OS default will be used. This parameter can be overridden at source level.+++ |  +++128+++ | {nbsp}
| Default send buffer size unit a| Enumeration, one of:

** BYTE
** KB
** MB
** GB |  +++The unit of measure for the send buffer size scalar. This parameter can be overridden at source level.+++ |  +++KB+++ | {nbsp}
| Default request timeout time unit a| Enumeration, one of:

** NANOSECONDS
** MICROSECONDS
** MILLISECONDS
** SECONDS
** MINUTES
** HOURS
** DAYS |  +++Determines the time unit for the request timeout time scalar.+++ |  +++SECONDS+++ | {nbsp}
| Default request timeout a| Number |  +++The configuration controls the maximum amount of time the client will wait for the response of a request. If the response is not received before the timeout elapses the client will resend the request if necessary or fail the request if retries are exhausted. This should be larger than replica.lag.time.max.ms (a broker configuration) to reduce the possibility of message duplication due to unnecessary producer retries.+++ |  +++30+++ | {nbsp}
| TLS Configuration a| <<Tls>> |  +++Defines a TLS configuration, which can be used from both the client and server sides to secure communication for the Mule app. The connector will automatically set the 'security.protocol' to use for communication. Valid values are PLAINTEXT / SSL / SASL_PLAINTEXT / SASL_SSL. Default value when no configuration has been provided is PLAINTEXT(or SASL_PLAINTEXT for SASL authentication - kerberos/scram/plain). If SSL was configured as protocol on the broker side then the user needs to configure at least the keystore in the 'tls:context' child element of this config and the connector will automatically use SSL(or SASL_SSL for SASL authentication) as 'security.protocol'.+++ |  | {nbsp}
| Principal a| String |  +++The entity that can be authenticated by a computer system or network. Principals can be individual people, computers, services, computational entities such as processes and threads, or any group of such things.+++ |  | *x*{nbsp}
| Service name a| String |  +++The Kerberos principal name that Kafka runs as.+++ |  | *x*{nbsp}
| Kerberos configuration file (krb5.conf) a| String |  +++The krb5.conf file contains Kerberos configuration information, including the locations of KDCs and admin servers for the Kerberos realms of interest, defaults for the current realm and for Kerberos applications, and mappings of hostnames onto Kerberos realms.+++ |  | {nbsp}
| Use ticket cache a| Boolean |  +++Set this to true, if you want the TGT to be obtained from the ticket cache. Set this option to false if you do not want to use the ticket cache. This connector will search for the ticket cache in the following locations: On Solaris and Linux it will look for the ticket cache in /tmp/krb5cc_uid where the uid is numeric user identifier. If the ticket cache is not available in the above location, or if we are on a Windows platform, it will look for the cache as {user.home}{file.separator}krb5cc_{user.name}. You can override the ticket cache location by using ticketCache. For Windows, if a ticket cannot be retrieved from the file ticket cache, it will use Local Security Authority (LSA) API to get the TGT.+++ |  +++false+++ | {nbsp}
| Ticket cache a| String |  +++Set this to the name of the ticket cache that contains the user's TGT. If this is set, useTicketCache must also be set to true; Otherwise a configuration error will be returned.+++ |  | {nbsp}
| Use keytab a| Boolean |  +++Set this to true if you want the module to get the principal's key from the the keytab. Default value is false. If keytab is not set then the module will locate the keytab from the Kerberos configuration file. If it is not specified in the Kerberos configuration file then it will look for the file {user.home}{file.separator}krb5.keytab.+++ |  +++false+++ | {nbsp}
| Keytab a| String |  +++Set this to the file name of the keytab to get the principal's secret key.+++ |  | {nbsp}
| Store key a| Boolean |  +++Set this to true if you want the principal's key to be stored in the Subject's private credentials.+++ |  +++false+++ | {nbsp}
| Reconnection a| <<Reconnection>> |  +++When the application is deployed, a connectivity test is performed on all connectors. If set to true, deployment will fail if the test doesn't pass after exhausting the associated reconnection strategy+++ |  | {nbsp}
|======================
[[producer-config_producer-sasl-plain-connection]]
===== Producer SASL/PLAIN Connection


====== Parameters
[cols=".^20%,.^20%,.^35%,.^20%,^.^5%", options="header"]
|======================
| Name | Type | Description | Default Value | Required
| Bootstrap Server URLs a| Array of String |  +++The list of servers to bootstrap the connection with the kafka cluster. This can be a partial list of the available servers.+++ |  | *x*{nbsp}
| Endpoint identification algorithm a| String |  +++The endpoint identification algorithm used by clients to validate server host name. The default value is an empty string, which means it is disabled. Clients including client connections created by the broker for inter-broker communication verify that the broker host name matches the host name in the brokers certificate.+++ |  | {nbsp}
| Batch size a| Number |  +++The producer will attempt to batch records together into fewer requests whenever multiple records are being sent to the same partition. This helps performance on both the client and the server. This configuration controls the default batch size in bytes. No attempt will be made to batch records larger than this size. Requests sent to brokers will contain multiple batches, one for each partition with data available to be sent. A small batch size will make batching less common and may reduce throughput (a batch size of zero will disable batching entirely). A very large batch size may use memory a bit more wastefully as we will always allocate a buffer of the specified batch size in anticipation of additional records.+++ |  +++16+++ | {nbsp}
| The batch size unit of measure. a| Enumeration, one of:

** BYTE
** KB
** MB
** GB |  +++The unit of measure for the batch size scalar.+++ |  +++KB+++ | {nbsp}
| Buffer size a| Number |  +++The total bytes of memory the producer can use to buffer records waiting to be sent to the server. If records are sent faster than they can be delivered to the server the producer will block for max.block.ms after which it will throw an exception. This setting should correspond roughly to the total memory the producer will use, but is not a hard bound since not all memory the producer uses is used for buffering. Some additional memory will be used for compression (if compression is enabled) as well as for maintaining in-flight requests. The default value in the Kafka docs is of 33554432 (32MB), but this should be capped to align with expected values for mule instances in cloudhub (v0.1 core)+++ |  +++1000+++ | {nbsp}
| The buffer memory size unit of measure. a| Enumeration, one of:

** BYTE
** KB
** MB
** GB |  +++The unit of measure for the max request size scalar.+++ |  +++KB+++ | {nbsp}
| DNS lookups a| Enumeration, one of:

** DEFAULT
** USE_ALL_DNS_IPS
** RESOLVE_CANONICAL_BOOTSTRAP_SERVERS_ONLY |  +++Controls how the client uses DNS lookups. If set to use_all_dns_ips then, when the lookup returns multiple IP addresses for a hostname, they will all be attempted to connect to before failing the connection. Applies to both bootstrap and advertised servers. If the value is resolve_canonical_bootstrap_servers_only each entry will be resolved and expanded into a list of canonical names.+++ |  +++DEFAULT+++ | {nbsp}
| Compression type a| Enumeration, one of:

** NONE
** GZIP
** SNAPPY
** LZ4
** ZSTD |  +++The compression type for all data generated by the producer. The default is none (i.e. no compression). Valid values are none, gzip, snappy, lz4, or zstd. Compression is of full batches of data, so the efficacy of batching will also impact the compression ratio (more batching means better compression).+++ |  +++NONE+++ | {nbsp}
| Connections maximum idle time a| Number |  +++Close idle connections after the value specified by this config.+++ |  +++540+++ | {nbsp}
| Connections maximum idle time unit a| Enumeration, one of:

** NANOSECONDS
** MICROSECONDS
** MILLISECONDS
** SECONDS
** MINUTES
** HOURS
** DAYS |  +++Determines the time unit for the connections maximum idle scalar.+++ |  +++SECONDS+++ | {nbsp}
| Delivery timeout a| Number |  +++An upper bound on the time to report success or failure after a call to send() returns. This limits the total time that a record will be delayed prior to sending, the time to await acknowledgement from the broker (if expected), and the time allowed for retriable send failures. The producer may report failure to send a record earlier than this config if either an unrecoverable error is encountered, the retries have been exhausted, or the record is added to a batch which reached an earlier delivery expiration deadline. The value of this config should be greater than or equal to the sum of request.timeout.ms and linger.ms.+++ |  +++120+++ | {nbsp}
| Delivery Timeout Time Unit a| Enumeration, one of:

** NANOSECONDS
** MICROSECONDS
** MILLISECONDS
** SECONDS
** MINUTES
** HOURS
** DAYS |  +++Determines the time unit for the delivery timeout scalar.+++ |  +++SECONDS+++ | {nbsp}
| Enable idempotence a| Boolean |  +++When set to 'true', the producer will ensure that exactly one copy of each message is written in the stream. If 'false', producer retries due to broker failures, etc, may write duplicates of the retried message in the stream. Note that enabling idempotence requires max.in.flight.requests.per.connection to be less than or equal to 5, retries to be greater than 0 and acks must be 'all'. If these values are not explicitly set by the user, suitable values will be chosen. If incompatible values are set, a ConnectionException will be thrown+++ |  +++false+++ | {nbsp}
| Linger time  a| Number |  +++The producer groups together any records that arrive in between request transmissions into a single batched request. Normally this occurs only under load when records arrive faster than they can be sent out. However in some circumstances the client may want to reduce the number of requests even under moderate load. This setting accomplishes this by adding a small amount of artificial delay?that is, rather than immediately sending out a record the producer will wait for up to the given delay to allow other records to be sent so that the sends can be batched together. This can be thought of as analogous to Nagle's algorithm in TCP. This setting gives the upper bound on the delay for batching: once we get batch.size worth of records for a partition it will be sent immediately regardless of this setting, however if we have fewer than this many bytes accumulated for this partition we will 'linger' for the specified time waiting for more records to show up. This setting defaults to 0 (i.e. no delay). Setting linger.ms=5, for example, would have the effect of reducing the number of requests sent but would add up to 5ms of latency to records sent in the absence of load.+++ |  +++0+++ | {nbsp}
| Linger Time Unit a| Enumeration, one of:

** NANOSECONDS
** MICROSECONDS
** MILLISECONDS
** SECONDS
** MINUTES
** HOURS
** DAYS |  +++Determines the time unit for the linger time scalar.+++ |  +++SECONDS+++ | {nbsp}
| Maximum block time  a| Number |  +++The configuration controls how long KafkaProducer.send() and KafkaProducer.partitionsFor() will block.These methods can be blocked either because the buffer is full or metadata unavailable.Blocking in the user-supplied serializers or partitioner will not be counted against this timeout.+++ |  +++60+++ | {nbsp}
| Maximum block time unit a| Enumeration, one of:

** NANOSECONDS
** MICROSECONDS
** MILLISECONDS
** SECONDS
** MINUTES
** HOURS
** DAYS |  +++Determines the time unit for the maximum block time scalar.+++ |  +++SECONDS+++ | {nbsp}
| Maximum in flight requests a| Number |  +++The maximum number of unacknowledged requests the client will send on a single connection before blocking. Note that if this setting is set to be greater than 1 and there are failed sends, there is a risk of message re-ordering due to retries (i.e., if retries are enabled).+++ |  +++5+++ | {nbsp}
| Maximum request size a| Number |  +++The maximum size of a request in bytes. This setting will limit the number of record batches the producer will send in a single request to avoid sending huge requests. This is also effectively a cap on the maximum record batch size. Note that the server has its own cap on record batch size which may be different from this.+++ |  +++1+++ | {nbsp}
| Maximum request size unit. a| Enumeration, one of:

** BYTE
** KB
** MB
** GB |  +++The unit of measure for the max request size scalar.+++ |  +++MB+++ | {nbsp}
| Producer acknowledge mode a| Enumeration, one of:

** NONE
** LEADER_ONLY
** ALL |  +++The number of acknowledgments the producer requires the leader to have received before considering a request complete. This controls the durability of records that are sent+++ |  +++NONE+++ | {nbsp}
| Default receive buffer size a| Number |  +++The size of the TCP receive buffer (SO_RCVBUF) to use when reading data. If the value is -1, the OS default will be used. This parameter can be overridden at source level.+++ |  +++64+++ | {nbsp}
| Default receive buffer size unit a| Enumeration, one of:

** BYTE
** KB
** MB
** GB |  +++The unit of measure for the receive buffer size scalar. This parameter can be overridden at source level.+++ |  +++KB+++ | {nbsp}
| Retries amount a| Number |  +++Setting a value greater than zero will cause the client to resend any record whose send fails with a potentially transient error. Note that this retry is no different than if the client resent the record upon receiving the error. Allowing retries without setting max.in.flight.requests.per.connection to 1 will potentially change the ordering of records because if two batches are sent to a single partition, and the first fails and is retried but the second succeeds, then the records in the second batch may appear first. Note additionally that produce requests will be failed before the number of retries has been exhausted if the timeout configured by delivery.timeout.ms expires first before successful acknowledgement. Users should generally prefer to leave this config unset and instead use delivery.timeout.ms to control retry behavior.+++ |  +++1+++ | {nbsp}
| Retry Backoff Timeout Time Unit a| Enumeration, one of:

** NANOSECONDS
** MICROSECONDS
** MILLISECONDS
** SECONDS
** MINUTES
** HOURS
** DAYS |  +++Determines the time unit for the retry backoff timeout time scalar.+++ |  +++MILLISECONDS+++ | {nbsp}
| Retry backoff timeout a| Number |  +++The amount of time to wait before attempting to retry a failed request to a given topic partition. This avoids repeatedly sending requests in a tight loop under some failure scenarios.+++ |  +++100+++ | {nbsp}
| Default send buffer size a| Number |  +++The size of the TCP send buffer (SO_SNDBUF) to use when sending data. If the value is -1, the OS default will be used. This parameter can be overridden at source level.+++ |  +++128+++ | {nbsp}
| Default send buffer size unit a| Enumeration, one of:

** BYTE
** KB
** MB
** GB |  +++The unit of measure for the send buffer size scalar. This parameter can be overridden at source level.+++ |  +++KB+++ | {nbsp}
| Default request timeout time unit a| Enumeration, one of:

** NANOSECONDS
** MICROSECONDS
** MILLISECONDS
** SECONDS
** MINUTES
** HOURS
** DAYS |  +++Determines the time unit for the request timeout time scalar.+++ |  +++SECONDS+++ | {nbsp}
| Default request timeout a| Number |  +++The configuration controls the maximum amount of time the client will wait for the response of a request. If the response is not received before the timeout elapses the client will resend the request if necessary or fail the request if retries are exhausted. This should be larger than replica.lag.time.max.ms (a broker configuration) to reduce the possibility of message duplication due to unnecessary producer retries.+++ |  +++30+++ | {nbsp}
| TLS Configuration a| <<Tls>> |  +++Defines a TLS configuration, which can be used from both the client and server sides to secure communication for the Mule app. The connector will automatically set the 'security.protocol' to use for communication. Valid values are PLAINTEXT / SSL / SASL_PLAINTEXT / SASL_SSL. Default value when no configuration has been provided is PLAINTEXT(or SASL_PLAINTEXT for SASL authentication - kerberos/scram/plain). If SSL was configured as protocol on the broker side then the user needs to configure at least the keystore in the 'tls:context' child element of this config and the connector will automatically use SSL(or SASL_SSL for SASL authentication) as 'security.protocol'.+++ |  | {nbsp}
| Username a| String |  +++The username with which to login.+++ |  | *x*{nbsp}
| Password a| String |  +++The password with which to login.+++ |  | *x*{nbsp}
| Reconnection a| <<Reconnection>> |  +++When the application is deployed, a connectivity test is performed on all connectors. If set to true, deployment will fail if the test doesn't pass after exhausting the associated reconnection strategy+++ |  | {nbsp}
|======================
[[producer-config_producer-sasl-scram-connection]]
===== Producer SASL/SCRAM Connection

+++
Salted Challenge Response Authentication Mechanism (SCRAM), or SASL/SCRAM, is a family of SASL mechanisms that addresses the security concerns with traditional mechanisms that perform username/password authentication like PLAIN. Apache KafkaÂ® supports SCRAM-SHA-256 and SCRAM-SHA-512.
+++

====== Parameters
[cols=".^20%,.^20%,.^35%,.^20%,^.^5%", options="header"]
|======================
| Name | Type | Description | Default Value | Required
| Bootstrap Server URLs a| Array of String |  +++The list of servers to bootstrap the connection with the kafka cluster. This can be a partial list of the available servers.+++ |  | *x*{nbsp}
| Endpoint identification algorithm a| String |  +++The endpoint identification algorithm used by clients to validate server host name. The default value is an empty string, which means it is disabled. Clients including client connections created by the broker for inter-broker communication verify that the broker host name matches the host name in the brokers certificate.+++ |  | {nbsp}
| Batch size a| Number |  +++The producer will attempt to batch records together into fewer requests whenever multiple records are being sent to the same partition. This helps performance on both the client and the server. This configuration controls the default batch size in bytes. No attempt will be made to batch records larger than this size. Requests sent to brokers will contain multiple batches, one for each partition with data available to be sent. A small batch size will make batching less common and may reduce throughput (a batch size of zero will disable batching entirely). A very large batch size may use memory a bit more wastefully as we will always allocate a buffer of the specified batch size in anticipation of additional records.+++ |  +++16+++ | {nbsp}
| The batch size unit of measure. a| Enumeration, one of:

** BYTE
** KB
** MB
** GB |  +++The unit of measure for the batch size scalar.+++ |  +++KB+++ | {nbsp}
| Buffer size a| Number |  +++The total bytes of memory the producer can use to buffer records waiting to be sent to the server. If records are sent faster than they can be delivered to the server the producer will block for max.block.ms after which it will throw an exception. This setting should correspond roughly to the total memory the producer will use, but is not a hard bound since not all memory the producer uses is used for buffering. Some additional memory will be used for compression (if compression is enabled) as well as for maintaining in-flight requests. The default value in the Kafka docs is of 33554432 (32MB), but this should be capped to align with expected values for mule instances in cloudhub (v0.1 core)+++ |  +++1000+++ | {nbsp}
| The buffer memory size unit of measure. a| Enumeration, one of:

** BYTE
** KB
** MB
** GB |  +++The unit of measure for the max request size scalar.+++ |  +++KB+++ | {nbsp}
| DNS lookups a| Enumeration, one of:

** DEFAULT
** USE_ALL_DNS_IPS
** RESOLVE_CANONICAL_BOOTSTRAP_SERVERS_ONLY |  +++Controls how the client uses DNS lookups. If set to use_all_dns_ips then, when the lookup returns multiple IP addresses for a hostname, they will all be attempted to connect to before failing the connection. Applies to both bootstrap and advertised servers. If the value is resolve_canonical_bootstrap_servers_only each entry will be resolved and expanded into a list of canonical names.+++ |  +++DEFAULT+++ | {nbsp}
| Compression type a| Enumeration, one of:

** NONE
** GZIP
** SNAPPY
** LZ4
** ZSTD |  +++The compression type for all data generated by the producer. The default is none (i.e. no compression). Valid values are none, gzip, snappy, lz4, or zstd. Compression is of full batches of data, so the efficacy of batching will also impact the compression ratio (more batching means better compression).+++ |  +++NONE+++ | {nbsp}
| Connections maximum idle time a| Number |  +++Close idle connections after the value specified by this config.+++ |  +++540+++ | {nbsp}
| Connections maximum idle time unit a| Enumeration, one of:

** NANOSECONDS
** MICROSECONDS
** MILLISECONDS
** SECONDS
** MINUTES
** HOURS
** DAYS |  +++Determines the time unit for the connections maximum idle scalar.+++ |  +++SECONDS+++ | {nbsp}
| Delivery timeout a| Number |  +++An upper bound on the time to report success or failure after a call to send() returns. This limits the total time that a record will be delayed prior to sending, the time to await acknowledgement from the broker (if expected), and the time allowed for retriable send failures. The producer may report failure to send a record earlier than this config if either an unrecoverable error is encountered, the retries have been exhausted, or the record is added to a batch which reached an earlier delivery expiration deadline. The value of this config should be greater than or equal to the sum of request.timeout.ms and linger.ms.+++ |  +++120+++ | {nbsp}
| Delivery Timeout Time Unit a| Enumeration, one of:

** NANOSECONDS
** MICROSECONDS
** MILLISECONDS
** SECONDS
** MINUTES
** HOURS
** DAYS |  +++Determines the time unit for the delivery timeout scalar.+++ |  +++SECONDS+++ | {nbsp}
| Enable idempotence a| Boolean |  +++When set to 'true', the producer will ensure that exactly one copy of each message is written in the stream. If 'false', producer retries due to broker failures, etc, may write duplicates of the retried message in the stream. Note that enabling idempotence requires max.in.flight.requests.per.connection to be less than or equal to 5, retries to be greater than 0 and acks must be 'all'. If these values are not explicitly set by the user, suitable values will be chosen. If incompatible values are set, a ConnectionException will be thrown+++ |  +++false+++ | {nbsp}
| Linger time  a| Number |  +++The producer groups together any records that arrive in between request transmissions into a single batched request. Normally this occurs only under load when records arrive faster than they can be sent out. However in some circumstances the client may want to reduce the number of requests even under moderate load. This setting accomplishes this by adding a small amount of artificial delay?that is, rather than immediately sending out a record the producer will wait for up to the given delay to allow other records to be sent so that the sends can be batched together. This can be thought of as analogous to Nagle's algorithm in TCP. This setting gives the upper bound on the delay for batching: once we get batch.size worth of records for a partition it will be sent immediately regardless of this setting, however if we have fewer than this many bytes accumulated for this partition we will 'linger' for the specified time waiting for more records to show up. This setting defaults to 0 (i.e. no delay). Setting linger.ms=5, for example, would have the effect of reducing the number of requests sent but would add up to 5ms of latency to records sent in the absence of load.+++ |  +++0+++ | {nbsp}
| Linger Time Unit a| Enumeration, one of:

** NANOSECONDS
** MICROSECONDS
** MILLISECONDS
** SECONDS
** MINUTES
** HOURS
** DAYS |  +++Determines the time unit for the linger time scalar.+++ |  +++SECONDS+++ | {nbsp}
| Maximum block time  a| Number |  +++The configuration controls how long KafkaProducer.send() and KafkaProducer.partitionsFor() will block.These methods can be blocked either because the buffer is full or metadata unavailable.Blocking in the user-supplied serializers or partitioner will not be counted against this timeout.+++ |  +++60+++ | {nbsp}
| Maximum block time unit a| Enumeration, one of:

** NANOSECONDS
** MICROSECONDS
** MILLISECONDS
** SECONDS
** MINUTES
** HOURS
** DAYS |  +++Determines the time unit for the maximum block time scalar.+++ |  +++SECONDS+++ | {nbsp}
| Maximum in flight requests a| Number |  +++The maximum number of unacknowledged requests the client will send on a single connection before blocking. Note that if this setting is set to be greater than 1 and there are failed sends, there is a risk of message re-ordering due to retries (i.e., if retries are enabled).+++ |  +++5+++ | {nbsp}
| Maximum request size a| Number |  +++The maximum size of a request in bytes. This setting will limit the number of record batches the producer will send in a single request to avoid sending huge requests. This is also effectively a cap on the maximum record batch size. Note that the server has its own cap on record batch size which may be different from this.+++ |  +++1+++ | {nbsp}
| Maximum request size unit. a| Enumeration, one of:

** BYTE
** KB
** MB
** GB |  +++The unit of measure for the max request size scalar.+++ |  +++MB+++ | {nbsp}
| Producer acknowledge mode a| Enumeration, one of:

** NONE
** LEADER_ONLY
** ALL |  +++The number of acknowledgments the producer requires the leader to have received before considering a request complete. This controls the durability of records that are sent+++ |  +++NONE+++ | {nbsp}
| Default receive buffer size a| Number |  +++The size of the TCP receive buffer (SO_RCVBUF) to use when reading data. If the value is -1, the OS default will be used. This parameter can be overridden at source level.+++ |  +++64+++ | {nbsp}
| Default receive buffer size unit a| Enumeration, one of:

** BYTE
** KB
** MB
** GB |  +++The unit of measure for the receive buffer size scalar. This parameter can be overridden at source level.+++ |  +++KB+++ | {nbsp}
| Retries amount a| Number |  +++Setting a value greater than zero will cause the client to resend any record whose send fails with a potentially transient error. Note that this retry is no different than if the client resent the record upon receiving the error. Allowing retries without setting max.in.flight.requests.per.connection to 1 will potentially change the ordering of records because if two batches are sent to a single partition, and the first fails and is retried but the second succeeds, then the records in the second batch may appear first. Note additionally that produce requests will be failed before the number of retries has been exhausted if the timeout configured by delivery.timeout.ms expires first before successful acknowledgement. Users should generally prefer to leave this config unset and instead use delivery.timeout.ms to control retry behavior.+++ |  +++1+++ | {nbsp}
| Retry Backoff Timeout Time Unit a| Enumeration, one of:

** NANOSECONDS
** MICROSECONDS
** MILLISECONDS
** SECONDS
** MINUTES
** HOURS
** DAYS |  +++Determines the time unit for the retry backoff timeout time scalar.+++ |  +++MILLISECONDS+++ | {nbsp}
| Retry backoff timeout a| Number |  +++The amount of time to wait before attempting to retry a failed request to a given topic partition. This avoids repeatedly sending requests in a tight loop under some failure scenarios.+++ |  +++100+++ | {nbsp}
| Default send buffer size a| Number |  +++The size of the TCP send buffer (SO_SNDBUF) to use when sending data. If the value is -1, the OS default will be used. This parameter can be overridden at source level.+++ |  +++128+++ | {nbsp}
| Default send buffer size unit a| Enumeration, one of:

** BYTE
** KB
** MB
** GB |  +++The unit of measure for the send buffer size scalar. This parameter can be overridden at source level.+++ |  +++KB+++ | {nbsp}
| Default request timeout time unit a| Enumeration, one of:

** NANOSECONDS
** MICROSECONDS
** MILLISECONDS
** SECONDS
** MINUTES
** HOURS
** DAYS |  +++Determines the time unit for the request timeout time scalar.+++ |  +++SECONDS+++ | {nbsp}
| Default request timeout a| Number |  +++The configuration controls the maximum amount of time the client will wait for the response of a request. If the response is not received before the timeout elapses the client will resend the request if necessary or fail the request if retries are exhausted. This should be larger than replica.lag.time.max.ms (a broker configuration) to reduce the possibility of message duplication due to unnecessary producer retries.+++ |  +++30+++ | {nbsp}
| TLS Configuration a| <<Tls>> |  +++Defines a TLS configuration, which can be used from both the client and server sides to secure communication for the Mule app. The connector will automatically set the 'security.protocol' to use for communication. Valid values are PLAINTEXT / SSL / SASL_PLAINTEXT / SASL_SSL. Default value when no configuration has been provided is PLAINTEXT(or SASL_PLAINTEXT for SASL authentication - kerberos/scram/plain). If SSL was configured as protocol on the broker side then the user needs to configure at least the keystore in the 'tls:context' child element of this config and the connector will automatically use SSL(or SASL_SSL for SASL authentication) as 'security.protocol'.+++ |  | {nbsp}
| Username a| String |  +++The username with which to login.+++ |  | *x*{nbsp}
| Password a| String |  +++The password with which to login.+++ |  | *x*{nbsp}
| Encryption type a| Enumeration, one of:

** SCRAM_SHA_256
** SCRAM_SHA_512 |  +++The encryption algorithm used by SCRAM. Only acceptable values are SCRAM_SHA_256 and SCRAM_SHA_512.+++ |  | *x*{nbsp}
| Reconnection a| <<Reconnection>> |  +++When the application is deployed, a connectivity test is performed on all connectors. If set to true, deployment will fail if the test doesn't pass after exhausting the associated reconnection strategy+++ |  | {nbsp}
|======================

==== Associated Operations
* <<publish>> {nbsp}



== Operations

[[commit]]
=== Commit
`<kafka:commit>`

+++
Commits the offsets associated to a message or batch of messages consumed in a Message Listener. This would be a List or a single message consumed in the BatchMessageListenerSource
+++

==== Parameters
[cols=".^20%,.^20%,.^35%,.^20%,^.^5%", options="header"]
|======================
| Name | Type | Description | Default Value | Required
| Configuration | String | The name of the configuration to use. | | *x*{nbsp}
| Consumer commit key a| String |  +++The commitKey of the last poll. This operation is only valid when used inside a flow that is using on of the MessageListenerSource(s) ( BatchMessageListenerSource / BatchMessageListenerSource) which insert this value as an attribute in the Mule Event+++ |  | *x*{nbsp}
| Reconnection Strategy a| * <<reconnect>>
* <<reconnect-forever>> |  +++A retry strategy in case of connectivity errors+++ |  | {nbsp}
|======================


==== For Configurations.
* <<consumer-config>> {nbsp}

==== Throws
* KAFKA:INVALID_ACK_MODE {nbsp}
* KAFKA:ALREADY_COMMITED {nbsp}
* KAFKA:RETRY_EXHAUSTED {nbsp}
* KAFKA:TIMEOUT {nbsp}
* KAFKA:SESSION_NOT_FOUND {nbsp}
* KAFKA:NOT_FOUND {nbsp}
* KAFKA:CONNECTIVITY {nbsp}


[[consume]]
=== Consume
`<kafka:consume>`

+++
This operation allows receiving messages from one or more Kafka topics, it works very similarly to the Message Listener source, so all the operations that apply to that, will apply to this operation as well. <p> Note: The consume operation works only in IMMEDIATE mode. The consume operation does not return the consumerCommitKey.
+++

==== Parameters
[cols=".^20%,.^20%,.^35%,.^20%,^.^5%", options="header"]
|======================
| Name | Type | Description | Default Value | Required
| Configuration | String | The name of the configuration to use. | | *x*{nbsp}
| Consumption timeout a| Number |  +++The amount of TimeUnits that this operation will wait for receiving messages.+++ |  | {nbsp}
| Timeout time unit a| Enumeration, one of:

** NANOSECONDS
** MICROSECONDS
** MILLISECONDS
** SECONDS
** MINUTES
** HOURS
** DAYS |  +++The unit of time for the timeout property.+++ |  | {nbsp}
| Operation Timeout a| Number |  |  | {nbsp}
| Operation Timeout Time Unit a| Enumeration, one of:

** NANOSECONDS
** MICROSECONDS
** MILLISECONDS
** SECONDS
** MINUTES
** HOURS
** DAYS |  |  | {nbsp}
| Acknowledgement mode a| Enumeration, one of:

** AUTO
** MANUAL
** IMMEDIATE
** DUPS_OK |  +++Declares the kind of Acknowledgement mode supported.+++ |  +++IMMEDIATE+++ | {nbsp}
| Streaming Strategy a| * <<repeatable-in-memory-stream>>
* <<repeatable-file-store-stream>>
* <<non-repeatable-stream>> |  +++Configure if repeatable streams should be used and their behaviour+++ |  | {nbsp}
| Target Variable a| String |  +++The name of a variable on which the operation's output will be placed+++ |  | {nbsp}
| Target Value a| String |  +++An expression that will be evaluated against the operation's output and the outcome of that expression will be stored in the target variable+++ |  +++#[payload]+++ | {nbsp}
| Reconnection Strategy a| * <<reconnect>>
* <<reconnect-forever>> |  +++A retry strategy in case of connectivity errors+++ |  | {nbsp}
|======================

==== Output
[cols=".^50%,.^50%"]
|======================
| *Type* a| Binary
| *Attributes Type* a| <<KafkaRecordAttributes>>
|======================

==== For Configurations.
* <<consumer-config>> {nbsp}

==== Throws
* KAFKA:RETRY_EXHAUSTED {nbsp}
* KAFKA:ILLEGAL_STATE {nbsp}
* KAFKA:TIMEOUT {nbsp}
* KAFKA:INVALID_OFFSET {nbsp}
* KAFKA:INVALID_INPUT {nbsp}
* KAFKA:NOT_FOUND {nbsp}
* KAFKA:CONNECTIVITY {nbsp}


[[seek]]
=== Seek
`<kafka:seek>`

+++
Sets the current offset of the consumer for the given topic and partition to the provided offset value.
+++

==== Parameters
[cols=".^20%,.^20%,.^35%,.^20%,^.^5%", options="header"]
|======================
| Name | Type | Description | Default Value | Required
| Configuration | String | The name of the configuration to use. | | *x*{nbsp}
| Topic a| String |  +++The name of the topic on which the seek operation will be performed.+++ |  | *x*{nbsp}
| Partition a| Number |  +++The partition number that will have its offset modified.+++ |  | *x*{nbsp}
| Offset a| Number |  +++The offset value to commit for the configured partition.+++ |  | *x*{nbsp}
| Operation Timeout a| Number |  |  | {nbsp}
| Operation Timeout Time Unit a| Enumeration, one of:

** NANOSECONDS
** MICROSECONDS
** MILLISECONDS
** SECONDS
** MINUTES
** HOURS
** DAYS |  |  | {nbsp}
| Reconnection Strategy a| * <<reconnect>>
* <<reconnect-forever>> |  +++A retry strategy in case of connectivity errors+++ |  | {nbsp}
|======================


==== For Configurations.
* <<consumer-config>> {nbsp}

==== Throws
* KAFKA:INVALID_TOPIC {nbsp}
* KAFKA:RETRY_EXHAUSTED {nbsp}
* KAFKA:TIMEOUT {nbsp}
* KAFKA:INVALID_OFFSET {nbsp}
* KAFKA:INVALID_INPUT {nbsp}
* KAFKA:NOT_FOUND {nbsp}
* KAFKA:CONNECTIVITY {nbsp}


[[publish]]
=== Publish
`<kafka:publish>`

+++
Publish a message to the specified kafka topic, optionally specifying the partition, key and message content for it. The publish operation supports transactions.
+++

==== Parameters
[cols=".^20%,.^20%,.^35%,.^20%,^.^5%", options="header"]
|======================
| Name | Type | Description | Default Value | Required
| Configuration | String | The name of the configuration to use. | | *x*{nbsp}
| Topic a| String |  +++The topic to publish to+++ |  | {nbsp}
| Partition a| Number |  +++Optional The partition tof+++ |  | {nbsp}
| Key a| Binary |  +++Optional key for the published message+++ |  | {nbsp}
| Message a| Binary |  +++Optional message content of the message+++ |  +++#[payload]+++ | {nbsp}
| Headers a| Object |  +++Optional headers for the message+++ |  | {nbsp}
| Transactional Action a| Enumeration, one of:

** ALWAYS_JOIN
** JOIN_IF_POSSIBLE
** NOT_SUPPORTED |  +++The type of joining action that operations can take regarding transactions.+++ |  +++JOIN_IF_POSSIBLE+++ | {nbsp}
| Target Variable a| String |  +++The name of a variable on which the operation's output will be placed+++ |  | {nbsp}
| Target Value a| String |  +++An expression that will be evaluated against the operation's output and the outcome of that expression will be stored in the target variable+++ |  +++#[payload]+++ | {nbsp}
| Reconnection Strategy a| * <<reconnect>>
* <<reconnect-forever>> |  +++A retry strategy in case of connectivity errors+++ |  | {nbsp}
|======================

==== Output
[cols=".^50%,.^50%"]
|======================
| *Type* a| <<KafkaMessageMetadata>>
|======================

==== For Configurations.
* <<producer-config>> {nbsp}

==== Throws
* KAFKA:INVALID_TOPIC_PARTITION {nbsp}
* KAFKA:RETRY_EXHAUSTED {nbsp}
* KAFKA:TIMEOUT {nbsp}
* KAFKA:INPUT_TOO_LARGE {nbsp}
* KAFKA:CONNECTIVITY {nbsp}


== Sources

[[batch-message-listener]]
=== Batch message listener
`<kafka:batch-message-listener>`

+++
This source supports the consumption of messages from a Kafka Cluster, producing a List of messages to the flow.
+++

==== Parameters
[cols=".^20%,.^20%,.^35%,.^20%,^.^5%", options="header"]
|======================
| Name | Type | Description | Default Value | Required
| Configuration | String | The name of the configuration to use. | | *x*{nbsp}
| Poll timeout a| Number |  +++TThe amount of time to wait for a response from the server.+++ |  | {nbsp}
| Poll timeout time unit a| Enumeration, one of:

** NANOSECONDS
** MICROSECONDS
** MILLISECONDS
** SECONDS
** MINUTES
** HOURS
** DAYS |  +++Determines the time unit for the poll timeout scalar.+++ |  | {nbsp}
| Acknowledgement mode a| Enumeration, one of:

** AUTO
** MANUAL
** IMMEDIATE
** DUPS_OK |  +++Declares the kind of Acknowledgement mode supported.+++ |  | {nbsp}
| Amount of parallel consumers a| Number |  |  +++1+++ | {nbsp}
| Primary Node Only a| Boolean |  +++Whether this source should only be executed on the primary node when runnning in Cluster+++ |  | {nbsp}
| Redelivery Policy a| <<RedeliveryPolicy>> |  +++Defines a policy for processing the redelivery of the same message+++ |  | {nbsp}
| Reconnection Strategy a| * <<reconnect>>
* <<reconnect-forever>> |  +++A retry strategy in case of connectivity errors+++ |  | {nbsp}
|======================

==== Output
[cols=".^50%,.^50%"]
|======================
| *Type* a| Array of <<Record>>
| *Attributes Type* a| <<ConsumerContext>>
|======================

==== For Configurations.
* <<consumer-config>> {nbsp}



[[message-listener]]
=== Message listener
`<kafka:message-listener>`

+++
This source supports the consumption of messages from a Kafka Cluster, producing a single message to the flow.
+++

==== Parameters
[cols=".^20%,.^20%,.^35%,.^20%,^.^5%", options="header"]
|======================
| Name | Type | Description | Default Value | Required
| Configuration | String | The name of the configuration to use. | | *x*{nbsp}
| Poll timeout a| Number |  +++TThe amount of time to wait for a response from the server.+++ |  | {nbsp}
| Poll timeout time unit a| Enumeration, one of:

** NANOSECONDS
** MICROSECONDS
** MILLISECONDS
** SECONDS
** MINUTES
** HOURS
** DAYS |  +++Determines the time unit for the poll timeout scalar.+++ |  | {nbsp}
| Acknowledgement mode a| Enumeration, one of:

** AUTO
** MANUAL
** IMMEDIATE
** DUPS_OK |  +++Declares the kind of Acknowledgement mode supported.+++ |  | {nbsp}
| Amount of parallel consumers a| Number |  |  +++1+++ | {nbsp}
| Primary Node Only a| Boolean |  +++Whether this source should only be executed on the primary node when runnning in Cluster+++ |  | {nbsp}
| Streaming Strategy a| * <<repeatable-in-memory-stream>>
* <<repeatable-file-store-stream>>
* <<non-repeatable-stream>> |  +++Configure if repeatable streams should be used and their behaviour+++ |  | {nbsp}
| Redelivery Policy a| <<RedeliveryPolicy>> |  +++Defines a policy for processing the redelivery of the same message+++ |  | {nbsp}
| Reconnection Strategy a| * <<reconnect>>
* <<reconnect-forever>> |  +++A retry strategy in case of connectivity errors+++ |  | {nbsp}
|======================

==== Output
[cols=".^50%,.^50%"]
|======================
| *Type* a| Binary
| *Attributes Type* a| <<KafkaRecordAttributes>>
|======================

==== For Configurations.
* <<consumer-config>> {nbsp}



== Types
[[Tls]]
=== Tls

[cols=".^20%,.^25%,.^30%,.^15%,.^10%", options="header"]
|======================
| Field | Type | Description | Default Value | Required
| Enabled Protocols a| String | A comma separated list of protocols enabled for this context. |  |
| Enabled Cipher Suites a| String | A comma separated list of cipher suites enabled for this context. |  |
| Trust Store a| <<TrustStore>> |  |  |
| Key Store a| <<KeyStore>> |  |  |
| Revocation Check a| * <<standard-revocation-check>>
* <<custom-ocsp-responder>>
* <<crl-file>> |  |  |
|======================

[[TrustStore]]
=== Trust Store

[cols=".^20%,.^25%,.^30%,.^15%,.^10%", options="header"]
|======================
| Field | Type | Description | Default Value | Required
| Path a| String | The location (which will be resolved relative to the current classpath and file system, if possible) of the trust store. |  |
| Password a| String | The password used to protect the trust store. |  |
| Type a| String | The type of store used. |  |
| Algorithm a| String | The algorithm used by the trust store. |  |
| Insecure a| Boolean | If true, no certificate validations will be performed, rendering connections vulnerable to attacks. Use at your own risk. |  |
|======================

[[KeyStore]]
=== Key Store

[cols=".^20%,.^25%,.^30%,.^15%,.^10%", options="header"]
|======================
| Field | Type | Description | Default Value | Required
| Path a| String | The location (which will be resolved relative to the current classpath and file system, if possible) of the key store. |  |
| Type a| String | The type of store used. |  |
| Alias a| String | When the key store contains many private keys, this attribute indicates the alias of the key that should be used. If not defined, the first key in the file will be used by default. |  |
| Key Password a| String | The password used to protect the private key. |  |
| Password a| String | The password used to protect the key store. |  |
| Algorithm a| String | The algorithm used by the key store. |  |
|======================

[[standard-revocation-check]]
=== Standard Revocation Check

[cols=".^20%,.^25%,.^30%,.^15%,.^10%", options="header"]
|======================
| Field | Type | Description | Default Value | Required
| Only End Entities a| Boolean | Only verify the last element of the certificate chain. |  |
| Prefer Crls a| Boolean | Try CRL instead of OCSP first. |  |
| No Fallback a| Boolean | Do not use the secondary checking method (the one not selected before). |  |
| Soft Fail a| Boolean | Avoid verification failure when the revocation server can not be reached or is busy. |  |
|======================

[[custom-ocsp-responder]]
=== Custom Ocsp Responder

[cols=".^20%,.^25%,.^30%,.^15%,.^10%", options="header"]
|======================
| Field | Type | Description | Default Value | Required
| Url a| String | The URL of the OCSP responder. |  |
| Cert Alias a| String | Alias of the signing certificate for the OCSP response (must be in the trust store), if present. |  |
|======================

[[crl-file]]
=== Crl File

[cols=".^20%,.^25%,.^30%,.^15%,.^10%", options="header"]
|======================
| Field | Type | Description | Default Value | Required
| Path a| String | The path to the CRL file. |  |
|======================

[[TopicPartition]]
=== Topic Partition

[cols=".^20%,.^25%,.^30%,.^15%,.^10%", options="header"]
|======================
| Field | Type | Description | Default Value | Required
| Topic a| String |  |  | x
| Partition a| Number |  |  | x
|======================

[[Reconnection]]
=== Reconnection

[cols=".^20%,.^25%,.^30%,.^15%,.^10%", options="header"]
|======================
| Field | Type | Description | Default Value | Required
| Fails Deployment a| Boolean | When the application is deployed, a connectivity test is performed on all connectors. If set to true, deployment will fail if the test doesn't pass after exhausting the associated reconnection strategy |  |
| Reconnection Strategy a| * <<reconnect>>
* <<reconnect-forever>> | The reconnection strategy to use |  |
|======================

[[reconnect]]
=== Reconnect

[cols=".^20%,.^25%,.^30%,.^15%,.^10%", options="header"]
|======================
| Field | Type | Description | Default Value | Required
| Frequency a| Number | How often (in ms) to reconnect |  |
| Count a| Number | How many reconnection attempts to make |  |
|======================

[[reconnect-forever]]
=== Reconnect Forever

[cols=".^20%,.^25%,.^30%,.^15%,.^10%", options="header"]
|======================
| Field | Type | Description | Default Value | Required
| Frequency a| Number | How often (in ms) to reconnect |  |
|======================

[[ExpirationPolicy]]
=== Expiration Policy

[cols=".^20%,.^25%,.^30%,.^15%,.^10%", options="header"]
|======================
| Field | Type | Description | Default Value | Required
| Max Idle Time a| Number | A scalar time value for the maximum amount of time a dynamic configuration instance should be allowed to be idle before it's considered eligible for expiration |  |
| Time Unit a| Enumeration, one of:

** NANOSECONDS
** MICROSECONDS
** MILLISECONDS
** SECONDS
** MINUTES
** HOURS
** DAYS | A time unit that qualifies the maxIdleTime attribute |  |
|======================

[[Record]]
=== Record

[cols=".^20%,.^25%,.^30%,.^15%,.^10%", options="header"]
|======================
| Field | Type | Description | Default Value | Required
| Attributes a| <<KafkaRecordAttributes>> |  |  |
| Payload a| Binary |  |  |
|======================

[[KafkaRecordAttributes]]
=== Kafka Record Attributes

[cols=".^20%,.^25%,.^30%,.^15%,.^10%", options="header"]
|======================
| Field | Type | Description | Default Value | Required
| Consumer Commit Key a| String |  |  |
| Creation Timestamp a| DateTime |  |  |
| Headers a| Object |  |  |
| Key a| Binary |  |  |
| Leader Epoch a| Number |  |  |
| Log Append Timestamp a| DateTime |  |  |
| Offset a| Number |  |  |
| Partition a| Number |  |  |
| Serialized Key Size a| Number |  |  |
| Serialized Value Size a| Number |  |  |
| Topic a| String |  |  |
|======================

[[ConsumerContext]]
=== Consumer Context

[cols=".^20%,.^25%,.^30%,.^15%,.^10%", options="header"]
|======================
| Field | Type | Description | Default Value | Required
| Consumer Commit Key a| String |  |  |
|======================

[[RedeliveryPolicy]]
=== Redelivery Policy

[cols=".^20%,.^25%,.^30%,.^15%,.^10%", options="header"]
|======================
| Field | Type | Description | Default Value | Required
| Max Redelivery Count a| Number | The maximum number of times a message can be redelivered and processed unsuccessfully before triggering process-failed-message |  |
| Use Secure Hash a| Boolean | Whether to use a secure hash algorithm to identify a redelivered message |  |
| Message Digest Algorithm a| String | The secure hashing algorithm to use. If not set, the default is SHA-256. |  |
| Id Expression a| String | Defines one or more expressions to use to determine when a message has been redelivered. This property may only be set if useSecureHash is false. |  |
| Object Store a| <<ObjectStore>> | The object store where the redelivery counter for each message is going to be stored. |  |
|======================

[[repeatable-in-memory-stream]]
=== Repeatable In Memory Stream

[cols=".^20%,.^25%,.^30%,.^15%,.^10%", options="header"]
|======================
| Field | Type | Description | Default Value | Required
| Initial Buffer Size a| Number | This is the amount of memory that will be allocated in order to consume the stream and provide random access to it. If the stream contains more data than can be fit into this buffer, then it will be expanded by according to the bufferSizeIncrement attribute, with an upper limit of maxInMemorySize. |  |
| Buffer Size Increment a| Number | This is by how much will be buffer size by expanded if it exceeds its initial size. Setting a value of zero or lower will mean that the buffer should not expand, meaning that a STREAM_MAXIMUM_SIZE_EXCEEDED error will be raised when the buffer gets full. |  |
| Max Buffer Size a| Number | This is the maximum amount of memory that will be used. If more than that is used then a STREAM_MAXIMUM_SIZE_EXCEEDED error will be raised. A value lower or equal to zero means no limit. |  |
| Buffer Unit a| Enumeration, one of:

** BYTE
** KB
** MB
** GB | The unit in which all these attributes are expressed |  |
|======================

[[repeatable-file-store-stream]]
=== Repeatable File Store Stream

[cols=".^20%,.^25%,.^30%,.^15%,.^10%", options="header"]
|======================
| Field | Type | Description | Default Value | Required
| In Memory Size a| Number | Defines the maximum memory that the stream should use to keep data in memory. If more than that is consumed then it will start to buffer the content on disk. |  |
| Buffer Unit a| Enumeration, one of:

** BYTE
** KB
** MB
** GB | The unit in which maxInMemorySize is expressed |  |
|======================

[[KafkaMessageMetadata]]
=== Kafka Message Metadata

[cols=".^20%,.^25%,.^30%,.^15%,.^10%", options="header"]
|======================
| Field | Type | Description | Default Value | Required
| Offset a| Number |  |  |
| Partition a| Number |  |  |
| Serialized Key Size a| Number |  |  |
| Serialized Value Size a| Number |  |  |
| Timestamp a| DateTime |  |  |
| Topic a| String |  |  |
|======================

== See Also

* xref:connectors::introduction/introduction-to-anypoint-connectors.adoc[Introduction to Anypoint Connectors]
* https://help.mulesoft.com[MuleSoft Help Center]
