
= Using Anypoint Studio to Configure MuleSoft Chain AI Connector 1.0.0 - Mule 4

include::connectors::partial$t-studio-partial.adoc[tags=topic-intro]


[[create-mule-project]]
== Create a Mule Project

include::connectors::partial$t-studio-partial.adoc[tags=create-project-intro]

[[add-connector-to-project]]
== Add the Connector to Your Mule Project

include::connectors::partial$t-studio-partial.adoc[tags=add-connector-to-project]

[[configure-source]]
== Configure a Source

include::connectors::partial$t-studio-partial.adoc[tags=configure-source-intro]
You can configure one of these sources to use with MuleSoft Chain AI Connector:

include::connectors::partial$t-studio-partial.adoc[tags=listener-scheduler-descriptions]


////
Use one source as an example. If the connector has a connector-specific source, use one of those sources as an example. In the example, list the required and important fields.  If the connector does not have a connector-specific source, use *HTTP > Listener*, using the partial shown below.
////

include::connectors::partial$t-studio-partial.adoc[tags=listener-steps]

[[add-connector-operation]]
== Add a Connector Operation to the Flow

include::connectors::partial$t-studio-partial.adoc[tags=add-connector-operation]

[[configure-global-element]]
== Configure a Global Element for the Connector

include::connectors::partial$t-studio-partial.adoc[tags=global-element-intro]

include::connectors::partial$t-studio-partial.adoc[tags=property-placeholders]

include::connectors::partial$t-studio-partial.adoc[tags=global-element-first-steps]

. In the *Global Elements Properties > General* tab, choose your preferred large language model (LLM) type from *LLM*:
+
** Anthropic
** Azure OpenAI
** Mistral AI
** Ollama
** OpenAI
** GroqAI
. From *Config type*, choose from these configuration types: 
* *Environment Variables*
+
This configuration requires you to set the environment variables in the operating system where Mule runtime is deployed. When you choose this option, enter a `-` in the *File path* field.
+
Based on the LLM type you chose, you must set different environment variables. These are the supported environment variables based on the supported LLM types:
+
* `# Anthropic: - ANTHROPIC_API_KEY` 
* `# AWS Bedrock:- AWS_ACCESS_KEY_ID- AWS_SECRET_ACCESS_KEY` 
* `# Azure OpenAI:- AZURE_OPENAI_ENDPOINT- AZURE_OPENAI_KEY- AZURE_OPENAI_DEPLOYMENT_NAME` 
* `# MistralAI:- MISTRAL_AI_API_KEY` 
* `# OpenAI:- OPENAI_API_KEY` 
* `# Ollama:- OLLAMA_BASE_URL` 
* `# Groq AI:- GROQ_API_KEY`

* *Configuration JSON* 
+
This configuration requires you to provide a configuration JSON file with all the required LLM properties.
+
This is an example of the configuration JSON file:
+
[source,json]
----
{
    "OPENAI": {
        "OPENAI_API_KEY": "YOUR_OPENAI_API_KEY"
    },
    "MISTRAL_AI": {
        "MISTRAL_AI_API_KEY": "YOUR_MISTRAL_AI_API_KEY"
    },
    "OLLAMA": {
        "OLLAMA_BASE_URL": "http://baseurl.ollama.com"
    },
    "GROQAI_OPENAI": {
        "GROQ_API_KEY": "YOUR_GROQAI_APIKEY"
    },
    "ANTHROPIC": {
        "ANTHROPIC_API_KEY": "YOUR_ANTHROPIC_API_KEY"
    },
    "AZURE_OPENAI": {
        "AZURE_OPENAI_KEY": "YOUR_AZURE_OPENAI_KEY",
        "AZURE_OPENAI_ENDPOINT": "http://endpoint.azure.com",
        "AZURE_OPENAI_DEPLOYMENT_NAME": "YOUR_DEPLOYMENT_NAME"
    }
}
----
+
Make sure you fill out the required properties for your LLM type. You can store the file externally or add it directly in the Mule application under the `src/main/resource` folder by using this Dataweave expression:
+
`DW Expression: mule.home ++ "/apps/" ++ app.name ++ "/envVars.json"`
. In *File path*, enter `-` if you chose *Environment Variables* as the configuration type. If you chose *Configuration JSON*, enter the path to the JSON file, or the Dataweave expression if you are storing it in the Mule application under the `src/main/resource` folder.
. In *Model name*, choose from the supported models for the LLM provider. 
. For *Temperature* enter a number between `0` and `2`, or use the default value `0.7`. 
+
The temperature is used to control the randomness of the output. When you set it higher, you'll get more random outputs. When you set it lower, towards `0`, the values are more deterministic.
. Enter the number of seconds for the *LLM Timeout* (when to time out the request). The default is `60` seconds. 
. *Max tokens* defines the number of LLM Token to use when generating a response. 
+
This parameter helps control the usage and costs when engaging with LLMs.
. Click *OK*.


== View the Application Log

include::connectors::partial$t-studio-partial.adoc[tags=view-app-log]


== Next Step

ifdef::additional-config[]
include::connectors::partial$t-studio-partial.adoc[tags=next-step-config-topics]
endif::[]

ifndef::additional-config[]
include::connectors::partial$t-studio-partial.adoc[tags=next-step-examples]
endif::[]


== See Also

include::connectors::partial$t-studio-partial.adoc[tags=see-also]
