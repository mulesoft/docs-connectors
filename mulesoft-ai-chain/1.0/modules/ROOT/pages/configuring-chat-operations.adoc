= Configuring Chat Operations

== Chat Answer Prompt

The *Chat answer prompt* operation is a simple prompt request operation to the configured LLM. It uses a plain text prompt as input and responds with a plain text answer.

=== Configure the Chat Answer Prompt Operation

To configure the *Chat answer prompt* operation:

. Select the operation on the Anypoint Code Builder or Studio canvas.
. In the *General* properties tab for the operation, enter plain text for the *Prompt*.

This is the XML configuration for this operation:

[[source,xml]]
----
<ms-chainai:chat-answer-prompt 
  doc:name="Chat answer prompt" 
  doc:id="8ba9d534-f801-4ac7-8a31-11462fc5204b" 
  config-ref="MuleChain_AI_Llm_configuration" 
  prompt="#[payload.prompt]"  
/>
----

[[output-config]]
=== Output Configuration

This operation responds with a JSON payload containing the main LLM response. 

This is an example response:

[source,json]
----
{
    "response": "The capital of Switzerland is Bern. It's known for its well-preserved medieval old town, which is a UNESCO World Heritage site. Bern became the capital of Switzerland in 1848. The Swiss parliament, the Federal Assembly, is located in Bern."
}
----

The operation also returns attributes that aren't within the main JSON payload, which include information about token usage, for example:

[source,json]
----
{
  "tokenUsage": {
      "outputCount": 9,
      "totalCount": 18,
      "inputCount": 9
  },
  "additionalAttributes": {}
}
----


== Configure the Chat Answer Prompt With Memory Operation

The *Chat answer prompt with memory* operation is useful for retaining conversation history for a multi-user chat operation.

To configure the *Chat answer prompt with memory* operation:

include::connectors::partial$t-configure-operations.adoc[]
* *Data*
+
Contains the prompt for the operation.
* *Memory Name*
+
Name of the conversation. For multi-user support, enter the unique user ID.
* *DB File Path* 
+
Path to the in-memory database for storing the conversation history. 
+
You can also use a DataWeave expression for this field, for example: 
+
`#["/Users/john.wick/Desktop/mac-demo/db/" ++ payload.memoryName].`
* Max Messages
+
Maximum number of messages to remember for the conversation defined in *Memory Name*.

This is the XML configuration for this operation:

[[source,xml]]
----
<ms-aichain:chat-answer-prompt-with-memory
  doc:name="Chat answer prompt with memory"
  doc:id="7e62e70e-eff7-4080-bb20-3d162bb84c39"
  config-ref="MuleSoft_AI_Chain_Config"
  memoryName="#[payload.memoryName]"
  dbFilePath='#["/Users/john.wick/Desktop/mac-demo/db/" ++ payload.memoryName]'
  maxMessages="#[payload.maxMessages]">
  <ms-aichain:data><![CDATA[#[payload.prompt]]]></ms-aichain:data>
</ms-aichain:chat-answer-prompt-with-memory>
----

=== Output Configuration

This operation responds with a JSON payload containing the main LLM response, with additional metadata stored in attributes.

This is an example response:

[source,json]
----
{
    "response": "I'm sorry, I do not have access to personal information such as your name."
}
----

The operation also returns attributes that aren't within the main JSON payload, which include information about token usage, for example:

[source,json]
----
{
  "tokenUsage": {
    "outputCount": 13,
    "totalCount": 44,
    "inputCount": 31
  },
  "additionalAttributes": {
    "memoryName": "memory",
    "maxMessages": "2",
    "dbFilePath": "/.../memory.db"
  }
}
----

== See Also

* xref:connectors::introduction/introduction-to-anypoint-connectors.adoc[Introduction to Anypoint Connectors]
* xref:index.adoc[]
* xref:mulesoft-ai-chain-connector-reference.adoc[]
* https://help.mulesoft.com[MuleSoft Help Center]


